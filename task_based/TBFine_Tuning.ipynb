{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f84d52-21dd-476e-9034-916a625a0f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import unsloth\n",
    "import torch\n",
    "from unsloth import FastLanguageModel  \n",
    "from transformers import AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "ckpt = \"checkpoint-41000\" # add the path to your own checkpoint \n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=ckpt, \n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False,\n",
    "    finetune_language_layers   = True,  \n",
    "    finetune_attention_modules = True,  \n",
    "    finetune_mlp_modules       = True, \n",
    "\n",
    "    r = 8,           \n",
    "    lora_alpha = 8,  \n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a7d25-5ab2-4811-ac38-81d6e124a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def get_legal_qa_dataset(json_path: str) -> Dataset:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    formatted = []\n",
    "\n",
    "    for item in raw_data:\n",
    "        question = item[\"question\"]\n",
    "        answer = item[\"answer\"]\n",
    "        reasoning = item[\"chain_of_thought\"]\n",
    "\n",
    "        formatted.append({\n",
    "            \"prompt\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": SYSTEM_PROMPT.strip()\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question\n",
    "                }\n",
    "            ],\n",
    "            \"answer\": f\"<reasoning>\\n{reasoning.strip()}\\n</reasoning>\\n<answer>\\n{answer.strip()}\\n</answer>\"\n",
    "        })\n",
    "\n",
    "    return Dataset.from_list(formatted)\n",
    "\n",
    "\n",
    "dataset = get_legal_qa_dataset(\"taskBased_input.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52631a4-c275-4330-b2ae-f166062c2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "max_prompt_length = 512  \n",
    "max_seq_length = 1024     \n",
    "\n",
    "from trl import GRPOConfig\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    report_to=\"wandb\",           \n",
    "    output_dir=\"./results\",\n",
    "\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_generations=2,                     \n",
    "    num_train_epochs=3,\n",
    "\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.06,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\" ,  \n",
    "    max_grad_norm=1.0,\n",
    "    fp16=False,\n",
    "    bf16=True\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc7e30-da94-4b7e-8ce1-48eabe80ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "model_embedder = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "def extract_parts(text: str) -> tuple[str, str]:\n",
    "    reasoning = re.search(r\"<reasoning>(.*?)</reasoning>\", text, re.DOTALL)\n",
    "    answer = re.search(r\"<answer>(.*?)</answer>\", text, re.DOTALL)\n",
    "    return (\n",
    "        reasoning.group(1).strip() if reasoning else \"\",\n",
    "        answer.group(1).strip() if answer else \"\"\n",
    "    )\n",
    "\n",
    "def format_reward_func(promt, completions, answer **kwargs) -> list[float]:\n",
    "    format_pattern = re.compile(r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\", re.DOTALL)\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = completion[0]['content']\n",
    "        if format_pattern.search(response):\n",
    "            scores.append(1.0)  \n",
    "        else:\n",
    "            scores.append(-0.5)  \n",
    "    return scores\n",
    "\n",
    "\n",
    "def simmilarity_reward_func(promt, completions, answer **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    reasoning_r, answer_r = zip(*[extract_parts(r) for r in responses])\n",
    "    reasoning_a, answer_a = zip(*[extract_parts(a) for a in answer])\n",
    "\n",
    "    reasoning_r = [f\"query: {r}\" for r in reasoning_r]\n",
    "    reasoning_a = [f\"query: {a}\" for a in reasoning_a]\n",
    "    answer_r = [f\"query: {r}\" for r in answer_r]\n",
    "    answer_a = [f\"query: {a}\" for a in answer_a]\n",
    "\n",
    "    emb_reasoning_r = model_embedder.encode(reasoning_r, convert_to_tensor=True)\n",
    "    emb_reasoning_a = model_embedder.encode(reasoning_a, convert_to_tensor=True)\n",
    "    emb_answer_r = model_embedder.encode(answer_r, convert_to_tensor=True)\n",
    "    emb_answer_a = model_embedder.encode(answer_a, convert_to_tensor=True)\n",
    "\n",
    "    sim_reasoning = util.pytorch_cos_sim(emb_reasoning_r, emb_reasoning_a).diag()\n",
    "    sim_answer = util.pytorch_cos_sim(emb_answer_r, emb_answer_a).diag()\n",
    "\n",
    "    rewards = [float((0.6 * r + 0.4 * a) * 2.0) for r, a in zip(sim_reasoning, sim_answer)]\n",
    "\n",
    "    wandb.log({\n",
    "        \"reward/reasoning_cosine_mean\": float(torch.mean(sim_reasoning)),\n",
    "        \"reward/answer_cosine_mean\": float(torch.mean(sim_answer)),\n",
    "        \"reward/final_reward_mean\": float(torch.mean(torch.tensor(rewards))),\n",
    "        \"reward/final_reward_std\": float(torch.std(torch.tensor(rewards))),\n",
    "    })\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164bbeb-abf5-48a1-a97e-8dc21a29ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOTrainer, GRPOConfig\n",
    "import wandb\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "wandb.init(\n",
    "    project=\"taskBasedFT\",   \n",
    "    resume=False,                   \n",
    "    reinit=True                       \n",
    ")\n",
    "\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "tokenizer.bos_token_id = tokenizer.bos_token_id or tokenizer.cls_token_id or 1\n",
    "tokenizer.eos_token_id = tokenizer.eos_token_id or tokenizer.sep_token_id or 2\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer, \n",
    "    reward_funcs=[\n",
    "        format_reward_func,\n",
    "        simmilarity_reward_func\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unsloth Torch 2.1.2",
   "language": "python",
   "name": "unsloth-torch21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
