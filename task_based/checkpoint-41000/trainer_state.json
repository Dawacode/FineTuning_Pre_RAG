{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.7645055797174742,
  "eval_steps": 500,
  "global_step": 41000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0033714304979602845,
      "grad_norm": 2.9540164470672607,
      "learning_rate": 2e-05,
      "loss": 3.5944,
      "step": 50
    },
    {
      "epoch": 0.006742860995920569,
      "grad_norm": 1.586977481842041,
      "learning_rate": 1.997749774977498e-05,
      "loss": 2.763,
      "step": 100
    },
    {
      "epoch": 0.010114291493880854,
      "grad_norm": 2.716172933578491,
      "learning_rate": 1.9954995499549957e-05,
      "loss": 2.4273,
      "step": 150
    },
    {
      "epoch": 0.013485721991841138,
      "grad_norm": 1.7267506122589111,
      "learning_rate": 1.9932493249324934e-05,
      "loss": 2.2719,
      "step": 200
    },
    {
      "epoch": 0.016857152489801422,
      "grad_norm": 2.2664263248443604,
      "learning_rate": 1.9909990999099912e-05,
      "loss": 2.1567,
      "step": 250
    },
    {
      "epoch": 0.020228582987761708,
      "grad_norm": 1.934967279434204,
      "learning_rate": 1.988748874887489e-05,
      "loss": 2.0112,
      "step": 300
    },
    {
      "epoch": 0.023600013485721993,
      "grad_norm": 1.671194076538086,
      "learning_rate": 1.9864986498649867e-05,
      "loss": 1.9253,
      "step": 350
    },
    {
      "epoch": 0.026971443983682276,
      "grad_norm": 2.0765857696533203,
      "learning_rate": 1.9842484248424844e-05,
      "loss": 1.9015,
      "step": 400
    },
    {
      "epoch": 0.03034287448164256,
      "grad_norm": 2.3993332386016846,
      "learning_rate": 1.9819981998199822e-05,
      "loss": 1.877,
      "step": 450
    },
    {
      "epoch": 0.033714304979602844,
      "grad_norm": 2.332840919494629,
      "learning_rate": 1.97974797479748e-05,
      "loss": 1.7929,
      "step": 500
    },
    {
      "epoch": 0.03708573547756313,
      "grad_norm": 2.38621187210083,
      "learning_rate": 1.9774977497749777e-05,
      "loss": 1.7979,
      "step": 550
    },
    {
      "epoch": 0.040457165975523415,
      "grad_norm": 2.465144395828247,
      "learning_rate": 1.9752475247524755e-05,
      "loss": 1.7316,
      "step": 600
    },
    {
      "epoch": 0.0438285964734837,
      "grad_norm": 2.497100591659546,
      "learning_rate": 1.9729972997299732e-05,
      "loss": 1.7098,
      "step": 650
    },
    {
      "epoch": 0.04720002697144399,
      "grad_norm": 2.909681797027588,
      "learning_rate": 1.970747074707471e-05,
      "loss": 1.6381,
      "step": 700
    },
    {
      "epoch": 0.050571457469404266,
      "grad_norm": 2.6417574882507324,
      "learning_rate": 1.9684968496849687e-05,
      "loss": 1.6563,
      "step": 750
    },
    {
      "epoch": 0.05394288796736455,
      "grad_norm": 2.7008793354034424,
      "learning_rate": 1.966246624662466e-05,
      "loss": 1.6206,
      "step": 800
    },
    {
      "epoch": 0.05731431846532484,
      "grad_norm": 3.282839775085449,
      "learning_rate": 1.9639963996399642e-05,
      "loss": 1.6026,
      "step": 850
    },
    {
      "epoch": 0.06068574896328512,
      "grad_norm": 3.101362466812134,
      "learning_rate": 1.961746174617462e-05,
      "loss": 1.5613,
      "step": 900
    },
    {
      "epoch": 0.06405717946124541,
      "grad_norm": 3.3602027893066406,
      "learning_rate": 1.9594959495949597e-05,
      "loss": 1.5637,
      "step": 950
    },
    {
      "epoch": 0.06742860995920569,
      "grad_norm": 2.961587905883789,
      "learning_rate": 1.9572457245724575e-05,
      "loss": 1.5099,
      "step": 1000
    },
    {
      "epoch": 0.07080004045716598,
      "grad_norm": 3.3460121154785156,
      "learning_rate": 1.9549954995499552e-05,
      "loss": 1.5108,
      "step": 1050
    },
    {
      "epoch": 0.07417147095512626,
      "grad_norm": 3.8219194412231445,
      "learning_rate": 1.952745274527453e-05,
      "loss": 1.4858,
      "step": 1100
    },
    {
      "epoch": 0.07754290145308654,
      "grad_norm": 4.037687301635742,
      "learning_rate": 1.9504950495049508e-05,
      "loss": 1.5056,
      "step": 1150
    },
    {
      "epoch": 0.08091433195104683,
      "grad_norm": 4.397822856903076,
      "learning_rate": 1.9482448244824485e-05,
      "loss": 1.4424,
      "step": 1200
    },
    {
      "epoch": 0.08428576244900711,
      "grad_norm": 4.354947566986084,
      "learning_rate": 1.945994599459946e-05,
      "loss": 1.4462,
      "step": 1250
    },
    {
      "epoch": 0.0876571929469674,
      "grad_norm": 2.901232957839966,
      "learning_rate": 1.9437443744374437e-05,
      "loss": 1.3559,
      "step": 1300
    },
    {
      "epoch": 0.09102862344492768,
      "grad_norm": 3.7544078826904297,
      "learning_rate": 1.9414941494149418e-05,
      "loss": 1.3845,
      "step": 1350
    },
    {
      "epoch": 0.09440005394288797,
      "grad_norm": 4.916205406188965,
      "learning_rate": 1.9392439243924395e-05,
      "loss": 1.3556,
      "step": 1400
    },
    {
      "epoch": 0.09777148444084825,
      "grad_norm": 3.8472347259521484,
      "learning_rate": 1.9369936993699373e-05,
      "loss": 1.3455,
      "step": 1450
    },
    {
      "epoch": 0.10114291493880853,
      "grad_norm": 4.611010551452637,
      "learning_rate": 1.934743474347435e-05,
      "loss": 1.3373,
      "step": 1500
    },
    {
      "epoch": 0.10451434543676882,
      "grad_norm": 3.2233808040618896,
      "learning_rate": 1.9324932493249328e-05,
      "loss": 1.3726,
      "step": 1550
    },
    {
      "epoch": 0.1078857759347291,
      "grad_norm": 4.174015522003174,
      "learning_rate": 1.9302430243024305e-05,
      "loss": 1.2665,
      "step": 1600
    },
    {
      "epoch": 0.1112572064326894,
      "grad_norm": 4.691556930541992,
      "learning_rate": 1.927992799279928e-05,
      "loss": 1.2242,
      "step": 1650
    },
    {
      "epoch": 0.11462863693064967,
      "grad_norm": 6.384541988372803,
      "learning_rate": 1.9257425742574257e-05,
      "loss": 1.3199,
      "step": 1700
    },
    {
      "epoch": 0.11800006742860995,
      "grad_norm": 6.428895473480225,
      "learning_rate": 1.9234923492349235e-05,
      "loss": 1.3141,
      "step": 1750
    },
    {
      "epoch": 0.12137149792657025,
      "grad_norm": 4.253359317779541,
      "learning_rate": 1.9212421242124212e-05,
      "loss": 1.303,
      "step": 1800
    },
    {
      "epoch": 0.12474292842453052,
      "grad_norm": 5.91556453704834,
      "learning_rate": 1.9189918991899193e-05,
      "loss": 1.3164,
      "step": 1850
    },
    {
      "epoch": 0.12811435892249082,
      "grad_norm": 4.549306392669678,
      "learning_rate": 1.916741674167417e-05,
      "loss": 1.1823,
      "step": 1900
    },
    {
      "epoch": 0.1314857894204511,
      "grad_norm": 12.94192886352539,
      "learning_rate": 1.9144914491449148e-05,
      "loss": 1.1778,
      "step": 1950
    },
    {
      "epoch": 0.13485721991841138,
      "grad_norm": 5.338071346282959,
      "learning_rate": 1.9122412241224126e-05,
      "loss": 1.2397,
      "step": 2000
    },
    {
      "epoch": 0.13822865041637167,
      "grad_norm": 7.141941547393799,
      "learning_rate": 1.90999099909991e-05,
      "loss": 1.1611,
      "step": 2050
    },
    {
      "epoch": 0.14160008091433196,
      "grad_norm": 5.490375518798828,
      "learning_rate": 1.9077407740774077e-05,
      "loss": 1.1791,
      "step": 2100
    },
    {
      "epoch": 0.14497151141229223,
      "grad_norm": 4.932700157165527,
      "learning_rate": 1.9054905490549055e-05,
      "loss": 1.2856,
      "step": 2150
    },
    {
      "epoch": 0.14834294191025252,
      "grad_norm": 4.372009754180908,
      "learning_rate": 1.9032403240324032e-05,
      "loss": 1.1271,
      "step": 2200
    },
    {
      "epoch": 0.1517143724082128,
      "grad_norm": 7.568317413330078,
      "learning_rate": 1.900990099009901e-05,
      "loss": 1.1657,
      "step": 2250
    },
    {
      "epoch": 0.15508580290617308,
      "grad_norm": 5.615792274475098,
      "learning_rate": 1.8987398739873988e-05,
      "loss": 1.1413,
      "step": 2300
    },
    {
      "epoch": 0.15845723340413337,
      "grad_norm": 5.746653079986572,
      "learning_rate": 1.896489648964897e-05,
      "loss": 1.1993,
      "step": 2350
    },
    {
      "epoch": 0.16182866390209366,
      "grad_norm": 5.275426864624023,
      "learning_rate": 1.8942394239423946e-05,
      "loss": 1.1574,
      "step": 2400
    },
    {
      "epoch": 0.16520009440005395,
      "grad_norm": 4.9629225730896,
      "learning_rate": 1.8919891989198924e-05,
      "loss": 1.0855,
      "step": 2450
    },
    {
      "epoch": 0.16857152489801422,
      "grad_norm": 5.650769233703613,
      "learning_rate": 1.8897389738973898e-05,
      "loss": 1.1605,
      "step": 2500
    },
    {
      "epoch": 0.1719429553959745,
      "grad_norm": 5.080548286437988,
      "learning_rate": 1.8874887488748875e-05,
      "loss": 1.0696,
      "step": 2550
    },
    {
      "epoch": 0.1753143858939348,
      "grad_norm": 5.380026817321777,
      "learning_rate": 1.8852385238523853e-05,
      "loss": 1.1532,
      "step": 2600
    },
    {
      "epoch": 0.17868581639189507,
      "grad_norm": 6.96160888671875,
      "learning_rate": 1.882988298829883e-05,
      "loss": 1.0626,
      "step": 2650
    },
    {
      "epoch": 0.18205724688985536,
      "grad_norm": 5.418126106262207,
      "learning_rate": 1.8807380738073808e-05,
      "loss": 1.0827,
      "step": 2700
    },
    {
      "epoch": 0.18542867738781565,
      "grad_norm": 5.496485710144043,
      "learning_rate": 1.8784878487848785e-05,
      "loss": 1.1004,
      "step": 2750
    },
    {
      "epoch": 0.18880010788577595,
      "grad_norm": 6.371413230895996,
      "learning_rate": 1.8762376237623763e-05,
      "loss": 1.0976,
      "step": 2800
    },
    {
      "epoch": 0.1921715383837362,
      "grad_norm": 7.294540882110596,
      "learning_rate": 1.8739873987398744e-05,
      "loss": 1.1348,
      "step": 2850
    },
    {
      "epoch": 0.1955429688816965,
      "grad_norm": 7.648758411407471,
      "learning_rate": 1.8717371737173718e-05,
      "loss": 1.068,
      "step": 2900
    },
    {
      "epoch": 0.1989143993796568,
      "grad_norm": 6.893930435180664,
      "learning_rate": 1.8694869486948696e-05,
      "loss": 1.088,
      "step": 2950
    },
    {
      "epoch": 0.20228582987761706,
      "grad_norm": 6.026394844055176,
      "learning_rate": 1.8672367236723673e-05,
      "loss": 1.0792,
      "step": 3000
    },
    {
      "epoch": 0.20565726037557736,
      "grad_norm": 7.906644821166992,
      "learning_rate": 1.864986498649865e-05,
      "loss": 1.1078,
      "step": 3050
    },
    {
      "epoch": 0.20902869087353765,
      "grad_norm": 5.376073837280273,
      "learning_rate": 1.8627362736273628e-05,
      "loss": 1.1249,
      "step": 3100
    },
    {
      "epoch": 0.2124001213714979,
      "grad_norm": 6.086310386657715,
      "learning_rate": 1.8604860486048606e-05,
      "loss": 1.0891,
      "step": 3150
    },
    {
      "epoch": 0.2157715518694582,
      "grad_norm": 6.265925884246826,
      "learning_rate": 1.8582358235823583e-05,
      "loss": 1.0566,
      "step": 3200
    },
    {
      "epoch": 0.2191429823674185,
      "grad_norm": 5.6509575843811035,
      "learning_rate": 1.855985598559856e-05,
      "loss": 1.045,
      "step": 3250
    },
    {
      "epoch": 0.2225144128653788,
      "grad_norm": 5.831485748291016,
      "learning_rate": 1.853735373537354e-05,
      "loss": 1.0193,
      "step": 3300
    },
    {
      "epoch": 0.22588584336333906,
      "grad_norm": 5.963380336761475,
      "learning_rate": 1.8514851485148516e-05,
      "loss": 1.016,
      "step": 3350
    },
    {
      "epoch": 0.22925727386129935,
      "grad_norm": 8.9813814163208,
      "learning_rate": 1.8492349234923493e-05,
      "loss": 0.9986,
      "step": 3400
    },
    {
      "epoch": 0.23262870435925964,
      "grad_norm": 8.016329765319824,
      "learning_rate": 1.846984698469847e-05,
      "loss": 1.0101,
      "step": 3450
    },
    {
      "epoch": 0.2360001348572199,
      "grad_norm": 5.0223917961120605,
      "learning_rate": 1.844734473447345e-05,
      "loss": 1.017,
      "step": 3500
    },
    {
      "epoch": 0.2393715653551802,
      "grad_norm": 7.2173027992248535,
      "learning_rate": 1.8424842484248426e-05,
      "loss": 1.0256,
      "step": 3550
    },
    {
      "epoch": 0.2427429958531405,
      "grad_norm": 7.940232276916504,
      "learning_rate": 1.8402340234023404e-05,
      "loss": 0.9237,
      "step": 3600
    },
    {
      "epoch": 0.24611442635110078,
      "grad_norm": 10.247949600219727,
      "learning_rate": 1.837983798379838e-05,
      "loss": 1.0636,
      "step": 3650
    },
    {
      "epoch": 0.24948585684906105,
      "grad_norm": 6.657578945159912,
      "learning_rate": 1.835733573357336e-05,
      "loss": 0.9594,
      "step": 3700
    },
    {
      "epoch": 0.25285728734702134,
      "grad_norm": 8.924930572509766,
      "learning_rate": 1.8334833483348336e-05,
      "loss": 0.9472,
      "step": 3750
    },
    {
      "epoch": 0.25622871784498163,
      "grad_norm": 7.365353107452393,
      "learning_rate": 1.8312331233123314e-05,
      "loss": 1.0078,
      "step": 3800
    },
    {
      "epoch": 0.2596001483429419,
      "grad_norm": 7.292423248291016,
      "learning_rate": 1.828982898289829e-05,
      "loss": 0.9816,
      "step": 3850
    },
    {
      "epoch": 0.2629715788409022,
      "grad_norm": 8.91215705871582,
      "learning_rate": 1.826732673267327e-05,
      "loss": 1.0049,
      "step": 3900
    },
    {
      "epoch": 0.26634300933886246,
      "grad_norm": 8.461590766906738,
      "learning_rate": 1.8244824482448246e-05,
      "loss": 0.9689,
      "step": 3950
    },
    {
      "epoch": 0.26971443983682275,
      "grad_norm": 9.255179405212402,
      "learning_rate": 1.8222322232223224e-05,
      "loss": 0.9958,
      "step": 4000
    },
    {
      "epoch": 0.27308587033478304,
      "grad_norm": 6.4071807861328125,
      "learning_rate": 1.81998199819982e-05,
      "loss": 1.0131,
      "step": 4050
    },
    {
      "epoch": 0.27645730083274334,
      "grad_norm": 7.064184665679932,
      "learning_rate": 1.817731773177318e-05,
      "loss": 0.9054,
      "step": 4100
    },
    {
      "epoch": 0.27982873133070363,
      "grad_norm": 5.247955799102783,
      "learning_rate": 1.8154815481548156e-05,
      "loss": 0.9672,
      "step": 4150
    },
    {
      "epoch": 0.2832001618286639,
      "grad_norm": 6.895049571990967,
      "learning_rate": 1.8132313231323134e-05,
      "loss": 0.9211,
      "step": 4200
    },
    {
      "epoch": 0.2865715923266242,
      "grad_norm": 5.469358921051025,
      "learning_rate": 1.810981098109811e-05,
      "loss": 1.0129,
      "step": 4250
    },
    {
      "epoch": 0.28994302282458445,
      "grad_norm": 7.790340423583984,
      "learning_rate": 1.808730873087309e-05,
      "loss": 0.9857,
      "step": 4300
    },
    {
      "epoch": 0.29331445332254474,
      "grad_norm": 7.849173069000244,
      "learning_rate": 1.8064806480648067e-05,
      "loss": 0.9035,
      "step": 4350
    },
    {
      "epoch": 0.29668588382050504,
      "grad_norm": 6.455327987670898,
      "learning_rate": 1.8042304230423044e-05,
      "loss": 0.8887,
      "step": 4400
    },
    {
      "epoch": 0.30005731431846533,
      "grad_norm": 6.971047878265381,
      "learning_rate": 1.8019801980198022e-05,
      "loss": 0.9479,
      "step": 4450
    },
    {
      "epoch": 0.3034287448164256,
      "grad_norm": 5.627532958984375,
      "learning_rate": 1.7997299729973e-05,
      "loss": 0.9189,
      "step": 4500
    },
    {
      "epoch": 0.3068001753143859,
      "grad_norm": 9.992980003356934,
      "learning_rate": 1.7974797479747977e-05,
      "loss": 0.9677,
      "step": 4550
    },
    {
      "epoch": 0.31017160581234615,
      "grad_norm": 5.831857204437256,
      "learning_rate": 1.7952295229522954e-05,
      "loss": 0.984,
      "step": 4600
    },
    {
      "epoch": 0.31354303631030644,
      "grad_norm": 8.830268859863281,
      "learning_rate": 1.7929792979297932e-05,
      "loss": 0.9928,
      "step": 4650
    },
    {
      "epoch": 0.31691446680826674,
      "grad_norm": 7.168679237365723,
      "learning_rate": 1.790729072907291e-05,
      "loss": 0.9504,
      "step": 4700
    },
    {
      "epoch": 0.32028589730622703,
      "grad_norm": 8.305965423583984,
      "learning_rate": 1.7884788478847887e-05,
      "loss": 0.9595,
      "step": 4750
    },
    {
      "epoch": 0.3236573278041873,
      "grad_norm": 8.378388404846191,
      "learning_rate": 1.786228622862286e-05,
      "loss": 0.9813,
      "step": 4800
    },
    {
      "epoch": 0.3270287583021476,
      "grad_norm": 6.855841636657715,
      "learning_rate": 1.7839783978397842e-05,
      "loss": 0.8961,
      "step": 4850
    },
    {
      "epoch": 0.3304001888001079,
      "grad_norm": 4.820126533508301,
      "learning_rate": 1.781728172817282e-05,
      "loss": 1.0055,
      "step": 4900
    },
    {
      "epoch": 0.33377161929806815,
      "grad_norm": 8.838981628417969,
      "learning_rate": 1.7794779477947797e-05,
      "loss": 0.9348,
      "step": 4950
    },
    {
      "epoch": 0.33714304979602844,
      "grad_norm": 12.839787483215332,
      "learning_rate": 1.7772277227722775e-05,
      "loss": 0.9039,
      "step": 5000
    },
    {
      "epoch": 0.34051448029398873,
      "grad_norm": 6.277414798736572,
      "learning_rate": 1.7749774977497752e-05,
      "loss": 0.9028,
      "step": 5050
    },
    {
      "epoch": 0.343885910791949,
      "grad_norm": 7.455502986907959,
      "learning_rate": 1.772727272727273e-05,
      "loss": 0.9138,
      "step": 5100
    },
    {
      "epoch": 0.3472573412899093,
      "grad_norm": 6.628906726837158,
      "learning_rate": 1.7704770477047707e-05,
      "loss": 0.9564,
      "step": 5150
    },
    {
      "epoch": 0.3506287717878696,
      "grad_norm": 7.381920337677002,
      "learning_rate": 1.768226822682268e-05,
      "loss": 0.976,
      "step": 5200
    },
    {
      "epoch": 0.3540002022858299,
      "grad_norm": 10.343493461608887,
      "learning_rate": 1.765976597659766e-05,
      "loss": 0.8895,
      "step": 5250
    },
    {
      "epoch": 0.35737163278379014,
      "grad_norm": 9.073345184326172,
      "learning_rate": 1.7637263726372636e-05,
      "loss": 0.8742,
      "step": 5300
    },
    {
      "epoch": 0.36074306328175043,
      "grad_norm": 9.582879066467285,
      "learning_rate": 1.7614761476147617e-05,
      "loss": 0.9382,
      "step": 5350
    },
    {
      "epoch": 0.3641144937797107,
      "grad_norm": 7.802623748779297,
      "learning_rate": 1.7592259225922595e-05,
      "loss": 0.9138,
      "step": 5400
    },
    {
      "epoch": 0.367485924277671,
      "grad_norm": 8.613301277160645,
      "learning_rate": 1.7569756975697572e-05,
      "loss": 0.8779,
      "step": 5450
    },
    {
      "epoch": 0.3708573547756313,
      "grad_norm": 8.523937225341797,
      "learning_rate": 1.754725472547255e-05,
      "loss": 0.9437,
      "step": 5500
    },
    {
      "epoch": 0.3742287852735916,
      "grad_norm": 8.286465644836426,
      "learning_rate": 1.7524752475247528e-05,
      "loss": 0.9238,
      "step": 5550
    },
    {
      "epoch": 0.3776002157715519,
      "grad_norm": 9.364020347595215,
      "learning_rate": 1.7502250225022505e-05,
      "loss": 0.8968,
      "step": 5600
    },
    {
      "epoch": 0.38097164626951213,
      "grad_norm": 6.681332111358643,
      "learning_rate": 1.747974797479748e-05,
      "loss": 0.8971,
      "step": 5650
    },
    {
      "epoch": 0.3843430767674724,
      "grad_norm": 8.688664436340332,
      "learning_rate": 1.7457245724572457e-05,
      "loss": 0.8849,
      "step": 5700
    },
    {
      "epoch": 0.3877145072654327,
      "grad_norm": 8.183866500854492,
      "learning_rate": 1.7434743474347434e-05,
      "loss": 0.9044,
      "step": 5750
    },
    {
      "epoch": 0.391085937763393,
      "grad_norm": 8.59097957611084,
      "learning_rate": 1.7412241224122412e-05,
      "loss": 0.8971,
      "step": 5800
    },
    {
      "epoch": 0.3944573682613533,
      "grad_norm": 15.413492202758789,
      "learning_rate": 1.7389738973897393e-05,
      "loss": 0.8488,
      "step": 5850
    },
    {
      "epoch": 0.3978287987593136,
      "grad_norm": 10.821351051330566,
      "learning_rate": 1.736723672367237e-05,
      "loss": 0.8317,
      "step": 5900
    },
    {
      "epoch": 0.4012002292572739,
      "grad_norm": 7.026340961456299,
      "learning_rate": 1.7344734473447348e-05,
      "loss": 0.8035,
      "step": 5950
    },
    {
      "epoch": 0.4045716597552341,
      "grad_norm": 8.077798843383789,
      "learning_rate": 1.7322232223222325e-05,
      "loss": 0.8642,
      "step": 6000
    },
    {
      "epoch": 0.4079430902531944,
      "grad_norm": 9.866026878356934,
      "learning_rate": 1.72997299729973e-05,
      "loss": 0.9491,
      "step": 6050
    },
    {
      "epoch": 0.4113145207511547,
      "grad_norm": 16.830286026000977,
      "learning_rate": 1.7277227722772277e-05,
      "loss": 0.8598,
      "step": 6100
    },
    {
      "epoch": 0.414685951249115,
      "grad_norm": 7.503278732299805,
      "learning_rate": 1.7254725472547255e-05,
      "loss": 0.8246,
      "step": 6150
    },
    {
      "epoch": 0.4180573817470753,
      "grad_norm": 9.88970947265625,
      "learning_rate": 1.7232223222322232e-05,
      "loss": 0.937,
      "step": 6200
    },
    {
      "epoch": 0.4214288122450356,
      "grad_norm": 8.952913284301758,
      "learning_rate": 1.720972097209721e-05,
      "loss": 0.8395,
      "step": 6250
    },
    {
      "epoch": 0.4248002427429958,
      "grad_norm": 6.058892250061035,
      "learning_rate": 1.7187218721872187e-05,
      "loss": 0.9246,
      "step": 6300
    },
    {
      "epoch": 0.4281716732409561,
      "grad_norm": 10.83866024017334,
      "learning_rate": 1.7164716471647168e-05,
      "loss": 0.8558,
      "step": 6350
    },
    {
      "epoch": 0.4315431037389164,
      "grad_norm": 7.779277801513672,
      "learning_rate": 1.7142214221422146e-05,
      "loss": 0.8364,
      "step": 6400
    },
    {
      "epoch": 0.4349145342368767,
      "grad_norm": 9.364448547363281,
      "learning_rate": 1.711971197119712e-05,
      "loss": 0.8473,
      "step": 6450
    },
    {
      "epoch": 0.438285964734837,
      "grad_norm": 10.103721618652344,
      "learning_rate": 1.7097209720972097e-05,
      "loss": 0.8378,
      "step": 6500
    },
    {
      "epoch": 0.4416573952327973,
      "grad_norm": 10.070165634155273,
      "learning_rate": 1.7074707470747075e-05,
      "loss": 0.8157,
      "step": 6550
    },
    {
      "epoch": 0.4450288257307576,
      "grad_norm": 10.696685791015625,
      "learning_rate": 1.7052205220522053e-05,
      "loss": 0.8476,
      "step": 6600
    },
    {
      "epoch": 0.4484002562287178,
      "grad_norm": 9.172574996948242,
      "learning_rate": 1.702970297029703e-05,
      "loss": 0.7782,
      "step": 6650
    },
    {
      "epoch": 0.4517716867266781,
      "grad_norm": 12.180331230163574,
      "learning_rate": 1.7007200720072008e-05,
      "loss": 0.8487,
      "step": 6700
    },
    {
      "epoch": 0.4551431172246384,
      "grad_norm": 7.623842239379883,
      "learning_rate": 1.6984698469846985e-05,
      "loss": 0.8438,
      "step": 6750
    },
    {
      "epoch": 0.4585145477225987,
      "grad_norm": 9.91796588897705,
      "learning_rate": 1.6962196219621963e-05,
      "loss": 0.8487,
      "step": 6800
    },
    {
      "epoch": 0.461885978220559,
      "grad_norm": 7.647099494934082,
      "learning_rate": 1.6939693969396944e-05,
      "loss": 0.8205,
      "step": 6850
    },
    {
      "epoch": 0.4652574087185193,
      "grad_norm": 7.736491680145264,
      "learning_rate": 1.6917191719171918e-05,
      "loss": 0.9051,
      "step": 6900
    },
    {
      "epoch": 0.4686288392164796,
      "grad_norm": 5.8321428298950195,
      "learning_rate": 1.6894689468946895e-05,
      "loss": 0.8265,
      "step": 6950
    },
    {
      "epoch": 0.4720002697144398,
      "grad_norm": 8.061195373535156,
      "learning_rate": 1.6872187218721873e-05,
      "loss": 0.8123,
      "step": 7000
    },
    {
      "epoch": 0.4753717002124001,
      "grad_norm": 6.521519660949707,
      "learning_rate": 1.684968496849685e-05,
      "loss": 0.8082,
      "step": 7050
    },
    {
      "epoch": 0.4787431307103604,
      "grad_norm": 5.292869567871094,
      "learning_rate": 1.6827182718271828e-05,
      "loss": 0.8061,
      "step": 7100
    },
    {
      "epoch": 0.4821145612083207,
      "grad_norm": 18.37018585205078,
      "learning_rate": 1.6804680468046805e-05,
      "loss": 0.8574,
      "step": 7150
    },
    {
      "epoch": 0.485485991706281,
      "grad_norm": 6.005259037017822,
      "learning_rate": 1.6782178217821783e-05,
      "loss": 0.7695,
      "step": 7200
    },
    {
      "epoch": 0.4888574222042413,
      "grad_norm": 10.366802215576172,
      "learning_rate": 1.675967596759676e-05,
      "loss": 0.8171,
      "step": 7250
    },
    {
      "epoch": 0.49222885270220157,
      "grad_norm": 9.895391464233398,
      "learning_rate": 1.6737173717371738e-05,
      "loss": 0.7825,
      "step": 7300
    },
    {
      "epoch": 0.4956002832001618,
      "grad_norm": 13.33935546875,
      "learning_rate": 1.6714671467146716e-05,
      "loss": 0.789,
      "step": 7350
    },
    {
      "epoch": 0.4989717136981221,
      "grad_norm": 8.193313598632812,
      "learning_rate": 1.6692169216921693e-05,
      "loss": 0.866,
      "step": 7400
    },
    {
      "epoch": 0.5023431441960824,
      "grad_norm": 6.7350311279296875,
      "learning_rate": 1.666966696669667e-05,
      "loss": 0.774,
      "step": 7450
    },
    {
      "epoch": 0.5057145746940427,
      "grad_norm": 9.221611976623535,
      "learning_rate": 1.6647164716471648e-05,
      "loss": 0.7717,
      "step": 7500
    },
    {
      "epoch": 0.509086005192003,
      "grad_norm": 10.474679946899414,
      "learning_rate": 1.6624662466246626e-05,
      "loss": 0.7866,
      "step": 7550
    },
    {
      "epoch": 0.5124574356899633,
      "grad_norm": 8.171089172363281,
      "learning_rate": 1.6602160216021603e-05,
      "loss": 0.7824,
      "step": 7600
    },
    {
      "epoch": 0.5158288661879236,
      "grad_norm": 7.8496413230896,
      "learning_rate": 1.657965796579658e-05,
      "loss": 0.8045,
      "step": 7650
    },
    {
      "epoch": 0.5192002966858839,
      "grad_norm": 7.652370452880859,
      "learning_rate": 1.655715571557156e-05,
      "loss": 0.8178,
      "step": 7700
    },
    {
      "epoch": 0.5225717271838441,
      "grad_norm": 12.30012321472168,
      "learning_rate": 1.6534653465346536e-05,
      "loss": 0.8222,
      "step": 7750
    },
    {
      "epoch": 0.5259431576818044,
      "grad_norm": 6.254553318023682,
      "learning_rate": 1.6512151215121513e-05,
      "loss": 0.7066,
      "step": 7800
    },
    {
      "epoch": 0.5293145881797646,
      "grad_norm": 6.963472843170166,
      "learning_rate": 1.648964896489649e-05,
      "loss": 0.7599,
      "step": 7850
    },
    {
      "epoch": 0.5326860186777249,
      "grad_norm": 7.573642253875732,
      "learning_rate": 1.646714671467147e-05,
      "loss": 0.8181,
      "step": 7900
    },
    {
      "epoch": 0.5360574491756852,
      "grad_norm": 9.667463302612305,
      "learning_rate": 1.6444644464446446e-05,
      "loss": 0.8547,
      "step": 7950
    },
    {
      "epoch": 0.5394288796736455,
      "grad_norm": 7.89066743850708,
      "learning_rate": 1.6422142214221424e-05,
      "loss": 0.7741,
      "step": 8000
    },
    {
      "epoch": 0.5428003101716058,
      "grad_norm": 9.644618034362793,
      "learning_rate": 1.63996399639964e-05,
      "loss": 0.8113,
      "step": 8050
    },
    {
      "epoch": 0.5461717406695661,
      "grad_norm": 9.890768051147461,
      "learning_rate": 1.637713771377138e-05,
      "loss": 0.7399,
      "step": 8100
    },
    {
      "epoch": 0.5495431711675264,
      "grad_norm": 9.176555633544922,
      "learning_rate": 1.6354635463546356e-05,
      "loss": 0.731,
      "step": 8150
    },
    {
      "epoch": 0.5529146016654867,
      "grad_norm": 8.667747497558594,
      "learning_rate": 1.6332133213321334e-05,
      "loss": 0.8111,
      "step": 8200
    },
    {
      "epoch": 0.556286032163447,
      "grad_norm": 14.170133590698242,
      "learning_rate": 1.630963096309631e-05,
      "loss": 0.8005,
      "step": 8250
    },
    {
      "epoch": 0.5596574626614073,
      "grad_norm": 7.625030994415283,
      "learning_rate": 1.628712871287129e-05,
      "loss": 0.77,
      "step": 8300
    },
    {
      "epoch": 0.5630288931593675,
      "grad_norm": 9.710955619812012,
      "learning_rate": 1.6264626462646266e-05,
      "loss": 0.8356,
      "step": 8350
    },
    {
      "epoch": 0.5664003236573278,
      "grad_norm": 12.265240669250488,
      "learning_rate": 1.6242124212421244e-05,
      "loss": 0.7824,
      "step": 8400
    },
    {
      "epoch": 0.5697717541552881,
      "grad_norm": 10.709955215454102,
      "learning_rate": 1.621962196219622e-05,
      "loss": 0.789,
      "step": 8450
    },
    {
      "epoch": 0.5731431846532484,
      "grad_norm": 8.91706657409668,
      "learning_rate": 1.61971197119712e-05,
      "loss": 0.8015,
      "step": 8500
    },
    {
      "epoch": 0.5765146151512086,
      "grad_norm": 9.577474594116211,
      "learning_rate": 1.6174617461746177e-05,
      "loss": 0.7709,
      "step": 8550
    },
    {
      "epoch": 0.5798860456491689,
      "grad_norm": 6.082174777984619,
      "learning_rate": 1.6152115211521154e-05,
      "loss": 0.7821,
      "step": 8600
    },
    {
      "epoch": 0.5832574761471292,
      "grad_norm": 7.549829483032227,
      "learning_rate": 1.612961296129613e-05,
      "loss": 0.7246,
      "step": 8650
    },
    {
      "epoch": 0.5866289066450895,
      "grad_norm": 9.06191635131836,
      "learning_rate": 1.610711071107111e-05,
      "loss": 0.8014,
      "step": 8700
    },
    {
      "epoch": 0.5900003371430498,
      "grad_norm": 8.482428550720215,
      "learning_rate": 1.6084608460846087e-05,
      "loss": 0.7993,
      "step": 8750
    },
    {
      "epoch": 0.5933717676410101,
      "grad_norm": 14.87674331665039,
      "learning_rate": 1.606210621062106e-05,
      "loss": 0.7205,
      "step": 8800
    },
    {
      "epoch": 0.5967431981389704,
      "grad_norm": 9.278157234191895,
      "learning_rate": 1.6039603960396042e-05,
      "loss": 0.775,
      "step": 8850
    },
    {
      "epoch": 0.6001146286369307,
      "grad_norm": 7.541731357574463,
      "learning_rate": 1.601710171017102e-05,
      "loss": 0.7856,
      "step": 8900
    },
    {
      "epoch": 0.603486059134891,
      "grad_norm": 6.866758823394775,
      "learning_rate": 1.5994599459945997e-05,
      "loss": 0.7152,
      "step": 8950
    },
    {
      "epoch": 0.6068574896328512,
      "grad_norm": 5.066478729248047,
      "learning_rate": 1.5972097209720974e-05,
      "loss": 0.7916,
      "step": 9000
    },
    {
      "epoch": 0.6102289201308115,
      "grad_norm": 5.058760643005371,
      "learning_rate": 1.5949594959495952e-05,
      "loss": 0.8328,
      "step": 9050
    },
    {
      "epoch": 0.6136003506287718,
      "grad_norm": 10.654033660888672,
      "learning_rate": 1.592709270927093e-05,
      "loss": 0.7118,
      "step": 9100
    },
    {
      "epoch": 0.6169717811267321,
      "grad_norm": 17.516830444335938,
      "learning_rate": 1.5904590459045907e-05,
      "loss": 0.7422,
      "step": 9150
    },
    {
      "epoch": 0.6203432116246923,
      "grad_norm": 11.639596939086914,
      "learning_rate": 1.588208820882088e-05,
      "loss": 0.8224,
      "step": 9200
    },
    {
      "epoch": 0.6237146421226526,
      "grad_norm": 8.51932144165039,
      "learning_rate": 1.585958595859586e-05,
      "loss": 0.7914,
      "step": 9250
    },
    {
      "epoch": 0.6270860726206129,
      "grad_norm": 10.352383613586426,
      "learning_rate": 1.5837083708370836e-05,
      "loss": 0.7769,
      "step": 9300
    },
    {
      "epoch": 0.6304575031185732,
      "grad_norm": 8.050915718078613,
      "learning_rate": 1.5814581458145817e-05,
      "loss": 0.7455,
      "step": 9350
    },
    {
      "epoch": 0.6338289336165335,
      "grad_norm": 22.316207885742188,
      "learning_rate": 1.5792079207920795e-05,
      "loss": 0.7339,
      "step": 9400
    },
    {
      "epoch": 0.6372003641144938,
      "grad_norm": 6.191457271575928,
      "learning_rate": 1.5769576957695772e-05,
      "loss": 0.7619,
      "step": 9450
    },
    {
      "epoch": 0.6405717946124541,
      "grad_norm": 6.57265567779541,
      "learning_rate": 1.574707470747075e-05,
      "loss": 0.6955,
      "step": 9500
    },
    {
      "epoch": 0.6439432251104144,
      "grad_norm": 14.309355735778809,
      "learning_rate": 1.5724572457245727e-05,
      "loss": 0.7257,
      "step": 9550
    },
    {
      "epoch": 0.6473146556083746,
      "grad_norm": 7.898675918579102,
      "learning_rate": 1.57020702070207e-05,
      "loss": 0.7795,
      "step": 9600
    },
    {
      "epoch": 0.6506860861063349,
      "grad_norm": 12.151942253112793,
      "learning_rate": 1.567956795679568e-05,
      "loss": 0.7177,
      "step": 9650
    },
    {
      "epoch": 0.6540575166042952,
      "grad_norm": 9.46961784362793,
      "learning_rate": 1.5657065706570657e-05,
      "loss": 0.7259,
      "step": 9700
    },
    {
      "epoch": 0.6574289471022555,
      "grad_norm": 9.206174850463867,
      "learning_rate": 1.5634563456345634e-05,
      "loss": 0.6998,
      "step": 9750
    },
    {
      "epoch": 0.6608003776002158,
      "grad_norm": 8.042789459228516,
      "learning_rate": 1.561206120612061e-05,
      "loss": 0.7417,
      "step": 9800
    },
    {
      "epoch": 0.6641718080981761,
      "grad_norm": 12.698429107666016,
      "learning_rate": 1.5589558955895593e-05,
      "loss": 0.7517,
      "step": 9850
    },
    {
      "epoch": 0.6675432385961363,
      "grad_norm": 8.220880508422852,
      "learning_rate": 1.556705670567057e-05,
      "loss": 0.7617,
      "step": 9900
    },
    {
      "epoch": 0.6709146690940966,
      "grad_norm": 11.66845417022705,
      "learning_rate": 1.5544554455445548e-05,
      "loss": 0.7172,
      "step": 9950
    },
    {
      "epoch": 0.6742860995920569,
      "grad_norm": 12.634061813354492,
      "learning_rate": 1.5522052205220525e-05,
      "loss": 0.6649,
      "step": 10000
    },
    {
      "epoch": 0.6776575300900172,
      "grad_norm": 10.073060035705566,
      "learning_rate": 1.54995499549955e-05,
      "loss": 0.712,
      "step": 10050
    },
    {
      "epoch": 0.6810289605879775,
      "grad_norm": 5.519107818603516,
      "learning_rate": 1.5477047704770477e-05,
      "loss": 0.67,
      "step": 10100
    },
    {
      "epoch": 0.6844003910859378,
      "grad_norm": 8.2369384765625,
      "learning_rate": 1.5454545454545454e-05,
      "loss": 0.7313,
      "step": 10150
    },
    {
      "epoch": 0.687771821583898,
      "grad_norm": 9.00851821899414,
      "learning_rate": 1.5432043204320432e-05,
      "loss": 0.7047,
      "step": 10200
    },
    {
      "epoch": 0.6911432520818583,
      "grad_norm": 10.461723327636719,
      "learning_rate": 1.540954095409541e-05,
      "loss": 0.7265,
      "step": 10250
    },
    {
      "epoch": 0.6945146825798186,
      "grad_norm": 8.125481605529785,
      "learning_rate": 1.5387038703870387e-05,
      "loss": 0.7971,
      "step": 10300
    },
    {
      "epoch": 0.6978861130777789,
      "grad_norm": 10.261514663696289,
      "learning_rate": 1.5364536453645368e-05,
      "loss": 0.7066,
      "step": 10350
    },
    {
      "epoch": 0.7012575435757392,
      "grad_norm": 13.542491912841797,
      "learning_rate": 1.5342034203420346e-05,
      "loss": 0.7467,
      "step": 10400
    },
    {
      "epoch": 0.7046289740736995,
      "grad_norm": 7.493788719177246,
      "learning_rate": 1.531953195319532e-05,
      "loss": 0.7491,
      "step": 10450
    },
    {
      "epoch": 0.7080004045716598,
      "grad_norm": 8.078566551208496,
      "learning_rate": 1.5297029702970297e-05,
      "loss": 0.7884,
      "step": 10500
    },
    {
      "epoch": 0.71137183506962,
      "grad_norm": 8.68307113647461,
      "learning_rate": 1.5274527452745275e-05,
      "loss": 0.7107,
      "step": 10550
    },
    {
      "epoch": 0.7147432655675803,
      "grad_norm": 7.909679412841797,
      "learning_rate": 1.5252025202520252e-05,
      "loss": 0.6698,
      "step": 10600
    },
    {
      "epoch": 0.7181146960655406,
      "grad_norm": 7.6579461097717285,
      "learning_rate": 1.522952295229523e-05,
      "loss": 0.7088,
      "step": 10650
    },
    {
      "epoch": 0.7214861265635009,
      "grad_norm": 14.6673002243042,
      "learning_rate": 1.5207020702070207e-05,
      "loss": 0.7761,
      "step": 10700
    },
    {
      "epoch": 0.7248575570614612,
      "grad_norm": 9.63772201538086,
      "learning_rate": 1.5184518451845185e-05,
      "loss": 0.7111,
      "step": 10750
    },
    {
      "epoch": 0.7282289875594214,
      "grad_norm": 10.650145530700684,
      "learning_rate": 1.5162016201620162e-05,
      "loss": 0.695,
      "step": 10800
    },
    {
      "epoch": 0.7316004180573817,
      "grad_norm": 10.27995777130127,
      "learning_rate": 1.5139513951395142e-05,
      "loss": 0.7621,
      "step": 10850
    },
    {
      "epoch": 0.734971848555342,
      "grad_norm": 7.9128737449646,
      "learning_rate": 1.511701170117012e-05,
      "loss": 0.7076,
      "step": 10900
    },
    {
      "epoch": 0.7383432790533023,
      "grad_norm": 10.316544532775879,
      "learning_rate": 1.5094509450945097e-05,
      "loss": 0.7054,
      "step": 10950
    },
    {
      "epoch": 0.7417147095512626,
      "grad_norm": 6.24891471862793,
      "learning_rate": 1.5072007200720074e-05,
      "loss": 0.7134,
      "step": 11000
    },
    {
      "epoch": 0.7450861400492229,
      "grad_norm": 6.474014759063721,
      "learning_rate": 1.504950495049505e-05,
      "loss": 0.6967,
      "step": 11050
    },
    {
      "epoch": 0.7484575705471832,
      "grad_norm": 8.313339233398438,
      "learning_rate": 1.5027002700270028e-05,
      "loss": 0.6997,
      "step": 11100
    },
    {
      "epoch": 0.7518290010451435,
      "grad_norm": 7.393439769744873,
      "learning_rate": 1.5004500450045005e-05,
      "loss": 0.6876,
      "step": 11150
    },
    {
      "epoch": 0.7552004315431038,
      "grad_norm": 8.634071350097656,
      "learning_rate": 1.4981998199819983e-05,
      "loss": 0.6835,
      "step": 11200
    },
    {
      "epoch": 0.758571862041064,
      "grad_norm": 7.586231708526611,
      "learning_rate": 1.495949594959496e-05,
      "loss": 0.6892,
      "step": 11250
    },
    {
      "epoch": 0.7619432925390243,
      "grad_norm": 8.91053295135498,
      "learning_rate": 1.4936993699369938e-05,
      "loss": 0.7225,
      "step": 11300
    },
    {
      "epoch": 0.7653147230369846,
      "grad_norm": 7.195159435272217,
      "learning_rate": 1.4914491449144917e-05,
      "loss": 0.7095,
      "step": 11350
    },
    {
      "epoch": 0.7686861535349448,
      "grad_norm": 6.639414310455322,
      "learning_rate": 1.4891989198919895e-05,
      "loss": 0.6958,
      "step": 11400
    },
    {
      "epoch": 0.7720575840329051,
      "grad_norm": 6.714478015899658,
      "learning_rate": 1.486948694869487e-05,
      "loss": 0.7034,
      "step": 11450
    },
    {
      "epoch": 0.7754290145308654,
      "grad_norm": 12.527647018432617,
      "learning_rate": 1.4846984698469848e-05,
      "loss": 0.6608,
      "step": 11500
    },
    {
      "epoch": 0.7788004450288257,
      "grad_norm": 12.72374439239502,
      "learning_rate": 1.4824482448244826e-05,
      "loss": 0.6942,
      "step": 11550
    },
    {
      "epoch": 0.782171875526786,
      "grad_norm": 6.764392375946045,
      "learning_rate": 1.4801980198019803e-05,
      "loss": 0.737,
      "step": 11600
    },
    {
      "epoch": 0.7855433060247463,
      "grad_norm": 12.045304298400879,
      "learning_rate": 1.477947794779478e-05,
      "loss": 0.6968,
      "step": 11650
    },
    {
      "epoch": 0.7889147365227066,
      "grad_norm": 9.908377647399902,
      "learning_rate": 1.4756975697569758e-05,
      "loss": 0.6366,
      "step": 11700
    },
    {
      "epoch": 0.7922861670206669,
      "grad_norm": 7.711785793304443,
      "learning_rate": 1.4734473447344736e-05,
      "loss": 0.6278,
      "step": 11750
    },
    {
      "epoch": 0.7956575975186272,
      "grad_norm": 10.764324188232422,
      "learning_rate": 1.4711971197119712e-05,
      "loss": 0.7342,
      "step": 11800
    },
    {
      "epoch": 0.7990290280165875,
      "grad_norm": 11.604473114013672,
      "learning_rate": 1.468946894689469e-05,
      "loss": 0.6556,
      "step": 11850
    },
    {
      "epoch": 0.8024004585145478,
      "grad_norm": 7.449032783508301,
      "learning_rate": 1.4666966696669668e-05,
      "loss": 0.6423,
      "step": 11900
    },
    {
      "epoch": 0.805771889012508,
      "grad_norm": 5.032982349395752,
      "learning_rate": 1.4644464446444646e-05,
      "loss": 0.6642,
      "step": 11950
    },
    {
      "epoch": 0.8091433195104683,
      "grad_norm": 8.510183334350586,
      "learning_rate": 1.4621962196219623e-05,
      "loss": 0.646,
      "step": 12000
    },
    {
      "epoch": 0.8125147500084285,
      "grad_norm": 8.013622283935547,
      "learning_rate": 1.4599459945994601e-05,
      "loss": 0.7226,
      "step": 12050
    },
    {
      "epoch": 0.8158861805063888,
      "grad_norm": 10.1101655960083,
      "learning_rate": 1.4576957695769578e-05,
      "loss": 0.6427,
      "step": 12100
    },
    {
      "epoch": 0.8192576110043491,
      "grad_norm": 8.003232955932617,
      "learning_rate": 1.4554455445544556e-05,
      "loss": 0.6675,
      "step": 12150
    },
    {
      "epoch": 0.8226290415023094,
      "grad_norm": 7.463110446929932,
      "learning_rate": 1.4531953195319532e-05,
      "loss": 0.6941,
      "step": 12200
    },
    {
      "epoch": 0.8260004720002697,
      "grad_norm": 9.769412994384766,
      "learning_rate": 1.450945094509451e-05,
      "loss": 0.6434,
      "step": 12250
    },
    {
      "epoch": 0.82937190249823,
      "grad_norm": 7.0373969078063965,
      "learning_rate": 1.4486948694869487e-05,
      "loss": 0.7224,
      "step": 12300
    },
    {
      "epoch": 0.8327433329961903,
      "grad_norm": 7.9562907218933105,
      "learning_rate": 1.4464446444644466e-05,
      "loss": 0.7039,
      "step": 12350
    },
    {
      "epoch": 0.8361147634941506,
      "grad_norm": 10.37798023223877,
      "learning_rate": 1.4441944194419444e-05,
      "loss": 0.7097,
      "step": 12400
    },
    {
      "epoch": 0.8394861939921109,
      "grad_norm": 11.17273998260498,
      "learning_rate": 1.4419441944194421e-05,
      "loss": 0.773,
      "step": 12450
    },
    {
      "epoch": 0.8428576244900712,
      "grad_norm": 6.8795576095581055,
      "learning_rate": 1.4396939693969399e-05,
      "loss": 0.7143,
      "step": 12500
    },
    {
      "epoch": 0.8462290549880315,
      "grad_norm": 9.647411346435547,
      "learning_rate": 1.4374437443744376e-05,
      "loss": 0.7073,
      "step": 12550
    },
    {
      "epoch": 0.8496004854859917,
      "grad_norm": 7.1430182456970215,
      "learning_rate": 1.4351935193519352e-05,
      "loss": 0.6701,
      "step": 12600
    },
    {
      "epoch": 0.852971915983952,
      "grad_norm": 12.062257766723633,
      "learning_rate": 1.432943294329433e-05,
      "loss": 0.6771,
      "step": 12650
    },
    {
      "epoch": 0.8563433464819122,
      "grad_norm": 8.219199180603027,
      "learning_rate": 1.4306930693069307e-05,
      "loss": 0.6533,
      "step": 12700
    },
    {
      "epoch": 0.8597147769798725,
      "grad_norm": 11.787138938903809,
      "learning_rate": 1.4284428442844285e-05,
      "loss": 0.6608,
      "step": 12750
    },
    {
      "epoch": 0.8630862074778328,
      "grad_norm": 11.636612892150879,
      "learning_rate": 1.4261926192619262e-05,
      "loss": 0.7202,
      "step": 12800
    },
    {
      "epoch": 0.8664576379757931,
      "grad_norm": 8.096120834350586,
      "learning_rate": 1.4239423942394242e-05,
      "loss": 0.7096,
      "step": 12850
    },
    {
      "epoch": 0.8698290684737534,
      "grad_norm": 15.307353973388672,
      "learning_rate": 1.4216921692169219e-05,
      "loss": 0.6441,
      "step": 12900
    },
    {
      "epoch": 0.8732004989717137,
      "grad_norm": 8.978395462036133,
      "learning_rate": 1.4194419441944197e-05,
      "loss": 0.6583,
      "step": 12950
    },
    {
      "epoch": 0.876571929469674,
      "grad_norm": 6.2314252853393555,
      "learning_rate": 1.4171917191719174e-05,
      "loss": 0.6158,
      "step": 13000
    },
    {
      "epoch": 0.8799433599676343,
      "grad_norm": 8.922744750976562,
      "learning_rate": 1.414941494149415e-05,
      "loss": 0.6053,
      "step": 13050
    },
    {
      "epoch": 0.8833147904655946,
      "grad_norm": 8.296609878540039,
      "learning_rate": 1.4126912691269128e-05,
      "loss": 0.694,
      "step": 13100
    },
    {
      "epoch": 0.8866862209635549,
      "grad_norm": 7.289190292358398,
      "learning_rate": 1.4104410441044105e-05,
      "loss": 0.6093,
      "step": 13150
    },
    {
      "epoch": 0.8900576514615152,
      "grad_norm": 10.490668296813965,
      "learning_rate": 1.4081908190819083e-05,
      "loss": 0.7228,
      "step": 13200
    },
    {
      "epoch": 0.8934290819594755,
      "grad_norm": 10.349491119384766,
      "learning_rate": 1.405940594059406e-05,
      "loss": 0.6737,
      "step": 13250
    },
    {
      "epoch": 0.8968005124574356,
      "grad_norm": 11.999977111816406,
      "learning_rate": 1.4036903690369038e-05,
      "loss": 0.6581,
      "step": 13300
    },
    {
      "epoch": 0.9001719429553959,
      "grad_norm": 13.59070873260498,
      "learning_rate": 1.4014401440144017e-05,
      "loss": 0.6467,
      "step": 13350
    },
    {
      "epoch": 0.9035433734533562,
      "grad_norm": 10.534393310546875,
      "learning_rate": 1.3991899189918994e-05,
      "loss": 0.7613,
      "step": 13400
    },
    {
      "epoch": 0.9069148039513165,
      "grad_norm": 8.280082702636719,
      "learning_rate": 1.396939693969397e-05,
      "loss": 0.6555,
      "step": 13450
    },
    {
      "epoch": 0.9102862344492768,
      "grad_norm": 8.290650367736816,
      "learning_rate": 1.3946894689468948e-05,
      "loss": 0.6488,
      "step": 13500
    },
    {
      "epoch": 0.9136576649472371,
      "grad_norm": 8.54118824005127,
      "learning_rate": 1.3924392439243925e-05,
      "loss": 0.6578,
      "step": 13550
    },
    {
      "epoch": 0.9170290954451974,
      "grad_norm": 6.328049659729004,
      "learning_rate": 1.3901890189018903e-05,
      "loss": 0.6663,
      "step": 13600
    },
    {
      "epoch": 0.9204005259431577,
      "grad_norm": 8.651565551757812,
      "learning_rate": 1.387938793879388e-05,
      "loss": 0.6614,
      "step": 13650
    },
    {
      "epoch": 0.923771956441118,
      "grad_norm": 10.173483848571777,
      "learning_rate": 1.3856885688568858e-05,
      "loss": 0.614,
      "step": 13700
    },
    {
      "epoch": 0.9271433869390783,
      "grad_norm": 9.586211204528809,
      "learning_rate": 1.3834383438343834e-05,
      "loss": 0.6656,
      "step": 13750
    },
    {
      "epoch": 0.9305148174370386,
      "grad_norm": 5.536608695983887,
      "learning_rate": 1.3811881188118811e-05,
      "loss": 0.6353,
      "step": 13800
    },
    {
      "epoch": 0.9338862479349989,
      "grad_norm": 7.1328301429748535,
      "learning_rate": 1.378937893789379e-05,
      "loss": 0.675,
      "step": 13850
    },
    {
      "epoch": 0.9372576784329592,
      "grad_norm": 18.179811477661133,
      "learning_rate": 1.3766876687668768e-05,
      "loss": 0.6366,
      "step": 13900
    },
    {
      "epoch": 0.9406291089309194,
      "grad_norm": 8.333515167236328,
      "learning_rate": 1.3744374437443746e-05,
      "loss": 0.6385,
      "step": 13950
    },
    {
      "epoch": 0.9440005394288796,
      "grad_norm": 12.377481460571289,
      "learning_rate": 1.3721872187218723e-05,
      "loss": 0.6517,
      "step": 14000
    },
    {
      "epoch": 0.9473719699268399,
      "grad_norm": 7.198650360107422,
      "learning_rate": 1.36993699369937e-05,
      "loss": 0.6129,
      "step": 14050
    },
    {
      "epoch": 0.9507434004248002,
      "grad_norm": 8.835602760314941,
      "learning_rate": 1.3676867686768678e-05,
      "loss": 0.7209,
      "step": 14100
    },
    {
      "epoch": 0.9541148309227605,
      "grad_norm": 10.681160926818848,
      "learning_rate": 1.3654365436543656e-05,
      "loss": 0.7054,
      "step": 14150
    },
    {
      "epoch": 0.9574862614207208,
      "grad_norm": 8.236970901489258,
      "learning_rate": 1.3631863186318632e-05,
      "loss": 0.5959,
      "step": 14200
    },
    {
      "epoch": 0.9608576919186811,
      "grad_norm": 6.366942882537842,
      "learning_rate": 1.360936093609361e-05,
      "loss": 0.5706,
      "step": 14250
    },
    {
      "epoch": 0.9642291224166414,
      "grad_norm": 8.622480392456055,
      "learning_rate": 1.3586858685868587e-05,
      "loss": 0.6234,
      "step": 14300
    },
    {
      "epoch": 0.9676005529146017,
      "grad_norm": 7.109364032745361,
      "learning_rate": 1.3564356435643566e-05,
      "loss": 0.6124,
      "step": 14350
    },
    {
      "epoch": 0.970971983412562,
      "grad_norm": 6.659186840057373,
      "learning_rate": 1.3541854185418544e-05,
      "loss": 0.7039,
      "step": 14400
    },
    {
      "epoch": 0.9743434139105223,
      "grad_norm": 12.42654037475586,
      "learning_rate": 1.3519351935193521e-05,
      "loss": 0.5899,
      "step": 14450
    },
    {
      "epoch": 0.9777148444084826,
      "grad_norm": 13.300878524780273,
      "learning_rate": 1.3496849684968499e-05,
      "loss": 0.6089,
      "step": 14500
    },
    {
      "epoch": 0.9810862749064428,
      "grad_norm": 9.414490699768066,
      "learning_rate": 1.3474347434743476e-05,
      "loss": 0.6646,
      "step": 14550
    },
    {
      "epoch": 0.9844577054044031,
      "grad_norm": 8.034345626831055,
      "learning_rate": 1.3451845184518452e-05,
      "loss": 0.5958,
      "step": 14600
    },
    {
      "epoch": 0.9878291359023633,
      "grad_norm": 8.035345077514648,
      "learning_rate": 1.342934293429343e-05,
      "loss": 0.6916,
      "step": 14650
    },
    {
      "epoch": 0.9912005664003236,
      "grad_norm": 10.75026798248291,
      "learning_rate": 1.3406840684068407e-05,
      "loss": 0.5966,
      "step": 14700
    },
    {
      "epoch": 0.9945719968982839,
      "grad_norm": 8.544289588928223,
      "learning_rate": 1.3384338433843385e-05,
      "loss": 0.6004,
      "step": 14750
    },
    {
      "epoch": 0.9979434273962442,
      "grad_norm": 7.443618297576904,
      "learning_rate": 1.3361836183618362e-05,
      "loss": 0.634,
      "step": 14800
    },
    {
      "epoch": 1.0012811435892248,
      "grad_norm": 8.137187957763672,
      "learning_rate": 1.3339333933393341e-05,
      "loss": 0.6379,
      "step": 14850
    },
    {
      "epoch": 1.0046525740871852,
      "grad_norm": 14.370591163635254,
      "learning_rate": 1.3316831683168319e-05,
      "loss": 0.6407,
      "step": 14900
    },
    {
      "epoch": 1.0080240045851454,
      "grad_norm": 13.068448066711426,
      "learning_rate": 1.3294329432943297e-05,
      "loss": 0.6208,
      "step": 14950
    },
    {
      "epoch": 1.0113954350831058,
      "grad_norm": 8.925958633422852,
      "learning_rate": 1.3271827182718272e-05,
      "loss": 0.585,
      "step": 15000
    },
    {
      "epoch": 1.014766865581066,
      "grad_norm": 6.919040203094482,
      "learning_rate": 1.324932493249325e-05,
      "loss": 0.5922,
      "step": 15050
    },
    {
      "epoch": 1.0181382960790264,
      "grad_norm": 9.843559265136719,
      "learning_rate": 1.3226822682268227e-05,
      "loss": 0.5864,
      "step": 15100
    },
    {
      "epoch": 1.0215097265769866,
      "grad_norm": 9.287038803100586,
      "learning_rate": 1.3204320432043205e-05,
      "loss": 0.6465,
      "step": 15150
    },
    {
      "epoch": 1.024881157074947,
      "grad_norm": 16.876815795898438,
      "learning_rate": 1.3181818181818183e-05,
      "loss": 0.6373,
      "step": 15200
    },
    {
      "epoch": 1.0282525875729072,
      "grad_norm": 10.013239860534668,
      "learning_rate": 1.315931593159316e-05,
      "loss": 0.6425,
      "step": 15250
    },
    {
      "epoch": 1.0316240180708676,
      "grad_norm": 8.520411491394043,
      "learning_rate": 1.3136813681368138e-05,
      "loss": 0.6017,
      "step": 15300
    },
    {
      "epoch": 1.0349954485688277,
      "grad_norm": 13.819644927978516,
      "learning_rate": 1.3114311431143117e-05,
      "loss": 0.6045,
      "step": 15350
    },
    {
      "epoch": 1.0383668790667882,
      "grad_norm": 6.216289520263672,
      "learning_rate": 1.3091809180918094e-05,
      "loss": 0.6114,
      "step": 15400
    },
    {
      "epoch": 1.0417383095647483,
      "grad_norm": 7.051040172576904,
      "learning_rate": 1.306930693069307e-05,
      "loss": 0.6078,
      "step": 15450
    },
    {
      "epoch": 1.0451097400627085,
      "grad_norm": 6.238004684448242,
      "learning_rate": 1.3046804680468048e-05,
      "loss": 0.6302,
      "step": 15500
    },
    {
      "epoch": 1.048481170560669,
      "grad_norm": 7.0050272941589355,
      "learning_rate": 1.3024302430243025e-05,
      "loss": 0.6435,
      "step": 15550
    },
    {
      "epoch": 1.051852601058629,
      "grad_norm": 10.555659294128418,
      "learning_rate": 1.3001800180018003e-05,
      "loss": 0.5702,
      "step": 15600
    },
    {
      "epoch": 1.0552240315565895,
      "grad_norm": 7.603295803070068,
      "learning_rate": 1.297929792979298e-05,
      "loss": 0.6144,
      "step": 15650
    },
    {
      "epoch": 1.0585954620545497,
      "grad_norm": 7.43892765045166,
      "learning_rate": 1.2956795679567958e-05,
      "loss": 0.603,
      "step": 15700
    },
    {
      "epoch": 1.06196689255251,
      "grad_norm": 9.109186172485352,
      "learning_rate": 1.2934293429342934e-05,
      "loss": 0.5895,
      "step": 15750
    },
    {
      "epoch": 1.0653383230504703,
      "grad_norm": 10.558982849121094,
      "learning_rate": 1.2911791179117911e-05,
      "loss": 0.6745,
      "step": 15800
    },
    {
      "epoch": 1.0687097535484307,
      "grad_norm": 9.2376127243042,
      "learning_rate": 1.288928892889289e-05,
      "loss": 0.5916,
      "step": 15850
    },
    {
      "epoch": 1.0720811840463909,
      "grad_norm": 8.989156723022461,
      "learning_rate": 1.2866786678667868e-05,
      "loss": 0.6376,
      "step": 15900
    },
    {
      "epoch": 1.0754526145443513,
      "grad_norm": 7.913039207458496,
      "learning_rate": 1.2844284428442846e-05,
      "loss": 0.6187,
      "step": 15950
    },
    {
      "epoch": 1.0788240450423114,
      "grad_norm": 8.797463417053223,
      "learning_rate": 1.2821782178217823e-05,
      "loss": 0.5977,
      "step": 16000
    },
    {
      "epoch": 1.0821954755402716,
      "grad_norm": 9.134237289428711,
      "learning_rate": 1.27992799279928e-05,
      "loss": 0.6334,
      "step": 16050
    },
    {
      "epoch": 1.085566906038232,
      "grad_norm": 11.015830039978027,
      "learning_rate": 1.2776777677767778e-05,
      "loss": 0.5907,
      "step": 16100
    },
    {
      "epoch": 1.0889383365361922,
      "grad_norm": 8.39991569519043,
      "learning_rate": 1.2754275427542756e-05,
      "loss": 0.6215,
      "step": 16150
    },
    {
      "epoch": 1.0923097670341526,
      "grad_norm": 9.403970718383789,
      "learning_rate": 1.2731773177317732e-05,
      "loss": 0.6126,
      "step": 16200
    },
    {
      "epoch": 1.0956811975321128,
      "grad_norm": 8.295811653137207,
      "learning_rate": 1.2709270927092709e-05,
      "loss": 0.6358,
      "step": 16250
    },
    {
      "epoch": 1.0990526280300732,
      "grad_norm": 11.083197593688965,
      "learning_rate": 1.2686768676867687e-05,
      "loss": 0.5552,
      "step": 16300
    },
    {
      "epoch": 1.1024240585280334,
      "grad_norm": 8.316914558410645,
      "learning_rate": 1.2664266426642666e-05,
      "loss": 0.6781,
      "step": 16350
    },
    {
      "epoch": 1.1057954890259938,
      "grad_norm": 8.122763633728027,
      "learning_rate": 1.2641764176417643e-05,
      "loss": 0.5619,
      "step": 16400
    },
    {
      "epoch": 1.109166919523954,
      "grad_norm": 4.913538455963135,
      "learning_rate": 1.2619261926192621e-05,
      "loss": 0.6149,
      "step": 16450
    },
    {
      "epoch": 1.1125383500219144,
      "grad_norm": 7.9355998039245605,
      "learning_rate": 1.2596759675967599e-05,
      "loss": 0.6411,
      "step": 16500
    },
    {
      "epoch": 1.1159097805198746,
      "grad_norm": 7.374227523803711,
      "learning_rate": 1.2574257425742576e-05,
      "loss": 0.5765,
      "step": 16550
    },
    {
      "epoch": 1.119281211017835,
      "grad_norm": 10.72617244720459,
      "learning_rate": 1.2551755175517552e-05,
      "loss": 0.5889,
      "step": 16600
    },
    {
      "epoch": 1.1226526415157951,
      "grad_norm": 5.354146957397461,
      "learning_rate": 1.252925292529253e-05,
      "loss": 0.6064,
      "step": 16650
    },
    {
      "epoch": 1.1260240720137555,
      "grad_norm": 7.301932334899902,
      "learning_rate": 1.2506750675067507e-05,
      "loss": 0.6269,
      "step": 16700
    },
    {
      "epoch": 1.1293955025117157,
      "grad_norm": 8.884137153625488,
      "learning_rate": 1.2484248424842485e-05,
      "loss": 0.6078,
      "step": 16750
    },
    {
      "epoch": 1.132766933009676,
      "grad_norm": 10.961172103881836,
      "learning_rate": 1.2461746174617462e-05,
      "loss": 0.6297,
      "step": 16800
    },
    {
      "epoch": 1.1361383635076363,
      "grad_norm": 12.448629379272461,
      "learning_rate": 1.2439243924392441e-05,
      "loss": 0.6325,
      "step": 16850
    },
    {
      "epoch": 1.1395097940055965,
      "grad_norm": 10.180049896240234,
      "learning_rate": 1.2416741674167419e-05,
      "loss": 0.6001,
      "step": 16900
    },
    {
      "epoch": 1.142881224503557,
      "grad_norm": 7.319782733917236,
      "learning_rate": 1.2394239423942396e-05,
      "loss": 0.6489,
      "step": 16950
    },
    {
      "epoch": 1.146252655001517,
      "grad_norm": 6.439899921417236,
      "learning_rate": 1.2371737173717372e-05,
      "loss": 0.619,
      "step": 17000
    },
    {
      "epoch": 1.1496240854994775,
      "grad_norm": 12.443892478942871,
      "learning_rate": 1.234923492349235e-05,
      "loss": 0.5918,
      "step": 17050
    },
    {
      "epoch": 1.1529955159974377,
      "grad_norm": 7.669743061065674,
      "learning_rate": 1.2326732673267327e-05,
      "loss": 0.5711,
      "step": 17100
    },
    {
      "epoch": 1.156366946495398,
      "grad_norm": 8.73275375366211,
      "learning_rate": 1.2304230423042305e-05,
      "loss": 0.5891,
      "step": 17150
    },
    {
      "epoch": 1.1597383769933582,
      "grad_norm": 10.395886421203613,
      "learning_rate": 1.2281728172817282e-05,
      "loss": 0.6539,
      "step": 17200
    },
    {
      "epoch": 1.1631098074913186,
      "grad_norm": 9.069883346557617,
      "learning_rate": 1.225922592259226e-05,
      "loss": 0.614,
      "step": 17250
    },
    {
      "epoch": 1.1664812379892788,
      "grad_norm": 8.670921325683594,
      "learning_rate": 1.2236723672367237e-05,
      "loss": 0.6002,
      "step": 17300
    },
    {
      "epoch": 1.169852668487239,
      "grad_norm": 5.7217020988464355,
      "learning_rate": 1.2214221422142217e-05,
      "loss": 0.6047,
      "step": 17350
    },
    {
      "epoch": 1.1732240989851994,
      "grad_norm": 7.799367904663086,
      "learning_rate": 1.2191719171917194e-05,
      "loss": 0.5775,
      "step": 17400
    },
    {
      "epoch": 1.1765955294831598,
      "grad_norm": 5.541708469390869,
      "learning_rate": 1.216921692169217e-05,
      "loss": 0.5642,
      "step": 17450
    },
    {
      "epoch": 1.17996695998112,
      "grad_norm": 8.557600975036621,
      "learning_rate": 1.2146714671467148e-05,
      "loss": 0.6039,
      "step": 17500
    },
    {
      "epoch": 1.1833383904790802,
      "grad_norm": 6.090925216674805,
      "learning_rate": 1.2124212421242125e-05,
      "loss": 0.6496,
      "step": 17550
    },
    {
      "epoch": 1.1867098209770406,
      "grad_norm": 8.503507614135742,
      "learning_rate": 1.2101710171017103e-05,
      "loss": 0.6328,
      "step": 17600
    },
    {
      "epoch": 1.1900812514750008,
      "grad_norm": 8.453547477722168,
      "learning_rate": 1.207920792079208e-05,
      "loss": 0.5705,
      "step": 17650
    },
    {
      "epoch": 1.1934526819729612,
      "grad_norm": 7.379645347595215,
      "learning_rate": 1.2056705670567058e-05,
      "loss": 0.5521,
      "step": 17700
    },
    {
      "epoch": 1.1968241124709214,
      "grad_norm": 8.202737808227539,
      "learning_rate": 1.2034203420342034e-05,
      "loss": 0.6238,
      "step": 17750
    },
    {
      "epoch": 1.2001955429688818,
      "grad_norm": 9.313277244567871,
      "learning_rate": 1.2011701170117011e-05,
      "loss": 0.5732,
      "step": 17800
    },
    {
      "epoch": 1.203566973466842,
      "grad_norm": 8.170415878295898,
      "learning_rate": 1.198919891989199e-05,
      "loss": 0.5803,
      "step": 17850
    },
    {
      "epoch": 1.2069384039648023,
      "grad_norm": 14.389400482177734,
      "learning_rate": 1.1966696669666968e-05,
      "loss": 0.6818,
      "step": 17900
    },
    {
      "epoch": 1.2103098344627625,
      "grad_norm": 8.315489768981934,
      "learning_rate": 1.1944194419441945e-05,
      "loss": 0.5417,
      "step": 17950
    },
    {
      "epoch": 1.213681264960723,
      "grad_norm": 10.018845558166504,
      "learning_rate": 1.1921692169216923e-05,
      "loss": 0.5901,
      "step": 18000
    },
    {
      "epoch": 1.217052695458683,
      "grad_norm": 10.671370506286621,
      "learning_rate": 1.18991899189919e-05,
      "loss": 0.5885,
      "step": 18050
    },
    {
      "epoch": 1.2204241259566433,
      "grad_norm": 8.38567066192627,
      "learning_rate": 1.1876687668766878e-05,
      "loss": 0.566,
      "step": 18100
    },
    {
      "epoch": 1.2237955564546037,
      "grad_norm": 9.364706993103027,
      "learning_rate": 1.1854185418541854e-05,
      "loss": 0.5496,
      "step": 18150
    },
    {
      "epoch": 1.2271669869525639,
      "grad_norm": 9.958060264587402,
      "learning_rate": 1.1831683168316831e-05,
      "loss": 0.5994,
      "step": 18200
    },
    {
      "epoch": 1.2305384174505243,
      "grad_norm": 10.019241333007812,
      "learning_rate": 1.1809180918091809e-05,
      "loss": 0.559,
      "step": 18250
    },
    {
      "epoch": 1.2339098479484845,
      "grad_norm": 7.6499457359313965,
      "learning_rate": 1.1786678667866787e-05,
      "loss": 0.5918,
      "step": 18300
    },
    {
      "epoch": 1.2372812784464449,
      "grad_norm": 15.070108413696289,
      "learning_rate": 1.1764176417641766e-05,
      "loss": 0.5407,
      "step": 18350
    },
    {
      "epoch": 1.240652708944405,
      "grad_norm": 4.893645286560059,
      "learning_rate": 1.1741674167416743e-05,
      "loss": 0.6725,
      "step": 18400
    },
    {
      "epoch": 1.2440241394423655,
      "grad_norm": 11.499573707580566,
      "learning_rate": 1.1719171917191721e-05,
      "loss": 0.6547,
      "step": 18450
    },
    {
      "epoch": 1.2473955699403256,
      "grad_norm": 9.861817359924316,
      "learning_rate": 1.1696669666966698e-05,
      "loss": 0.6364,
      "step": 18500
    },
    {
      "epoch": 1.250767000438286,
      "grad_norm": 8.40566635131836,
      "learning_rate": 1.1674167416741676e-05,
      "loss": 0.5931,
      "step": 18550
    },
    {
      "epoch": 1.2541384309362462,
      "grad_norm": 8.543519973754883,
      "learning_rate": 1.1651665166516652e-05,
      "loss": 0.6314,
      "step": 18600
    },
    {
      "epoch": 1.2575098614342064,
      "grad_norm": 6.321774959564209,
      "learning_rate": 1.162916291629163e-05,
      "loss": 0.573,
      "step": 18650
    },
    {
      "epoch": 1.2608812919321668,
      "grad_norm": 8.281987190246582,
      "learning_rate": 1.1606660666066607e-05,
      "loss": 0.5916,
      "step": 18700
    },
    {
      "epoch": 1.2642527224301272,
      "grad_norm": 8.07558822631836,
      "learning_rate": 1.1584158415841584e-05,
      "loss": 0.5002,
      "step": 18750
    },
    {
      "epoch": 1.2676241529280874,
      "grad_norm": 7.157734394073486,
      "learning_rate": 1.1561656165616562e-05,
      "loss": 0.5745,
      "step": 18800
    },
    {
      "epoch": 1.2709955834260476,
      "grad_norm": 10.826438903808594,
      "learning_rate": 1.1539153915391541e-05,
      "loss": 0.5748,
      "step": 18850
    },
    {
      "epoch": 1.274367013924008,
      "grad_norm": 5.572880268096924,
      "learning_rate": 1.1516651665166519e-05,
      "loss": 0.5952,
      "step": 18900
    },
    {
      "epoch": 1.2777384444219684,
      "grad_norm": 7.296029567718506,
      "learning_rate": 1.1494149414941496e-05,
      "loss": 0.6659,
      "step": 18950
    },
    {
      "epoch": 1.2811098749199286,
      "grad_norm": 8.6798095703125,
      "learning_rate": 1.1471647164716472e-05,
      "loss": 0.6106,
      "step": 19000
    },
    {
      "epoch": 1.2844813054178887,
      "grad_norm": 6.696327209472656,
      "learning_rate": 1.144914491449145e-05,
      "loss": 0.5665,
      "step": 19050
    },
    {
      "epoch": 1.2878527359158491,
      "grad_norm": 7.5582146644592285,
      "learning_rate": 1.1426642664266427e-05,
      "loss": 0.5647,
      "step": 19100
    },
    {
      "epoch": 1.2912241664138093,
      "grad_norm": 13.061091423034668,
      "learning_rate": 1.1404140414041405e-05,
      "loss": 0.6129,
      "step": 19150
    },
    {
      "epoch": 1.2945955969117697,
      "grad_norm": 8.620635986328125,
      "learning_rate": 1.1381638163816382e-05,
      "loss": 0.5886,
      "step": 19200
    },
    {
      "epoch": 1.29796702740973,
      "grad_norm": 4.607493877410889,
      "learning_rate": 1.135913591359136e-05,
      "loss": 0.5714,
      "step": 19250
    },
    {
      "epoch": 1.3013384579076903,
      "grad_norm": 13.902115821838379,
      "learning_rate": 1.1336633663366337e-05,
      "loss": 0.6356,
      "step": 19300
    },
    {
      "epoch": 1.3047098884056505,
      "grad_norm": 5.365721702575684,
      "learning_rate": 1.1314131413141317e-05,
      "loss": 0.5901,
      "step": 19350
    },
    {
      "epoch": 1.3080813189036107,
      "grad_norm": 10.514272689819336,
      "learning_rate": 1.1291629162916294e-05,
      "loss": 0.5804,
      "step": 19400
    },
    {
      "epoch": 1.311452749401571,
      "grad_norm": 9.556684494018555,
      "learning_rate": 1.126912691269127e-05,
      "loss": 0.6116,
      "step": 19450
    },
    {
      "epoch": 1.3148241798995315,
      "grad_norm": 6.794490337371826,
      "learning_rate": 1.1246624662466248e-05,
      "loss": 0.642,
      "step": 19500
    },
    {
      "epoch": 1.3181956103974917,
      "grad_norm": 8.35971736907959,
      "learning_rate": 1.1224122412241225e-05,
      "loss": 0.5158,
      "step": 19550
    },
    {
      "epoch": 1.3215670408954518,
      "grad_norm": 10.602102279663086,
      "learning_rate": 1.1201620162016203e-05,
      "loss": 0.5483,
      "step": 19600
    },
    {
      "epoch": 1.3249384713934123,
      "grad_norm": 7.860709190368652,
      "learning_rate": 1.117911791179118e-05,
      "loss": 0.6114,
      "step": 19650
    },
    {
      "epoch": 1.3283099018913724,
      "grad_norm": 12.925374984741211,
      "learning_rate": 1.1156615661566158e-05,
      "loss": 0.5525,
      "step": 19700
    },
    {
      "epoch": 1.3316813323893328,
      "grad_norm": 8.022937774658203,
      "learning_rate": 1.1134113411341134e-05,
      "loss": 0.6087,
      "step": 19750
    },
    {
      "epoch": 1.335052762887293,
      "grad_norm": 6.704073429107666,
      "learning_rate": 1.1111611161116111e-05,
      "loss": 0.6217,
      "step": 19800
    },
    {
      "epoch": 1.3384241933852534,
      "grad_norm": 8.322348594665527,
      "learning_rate": 1.108910891089109e-05,
      "loss": 0.5696,
      "step": 19850
    },
    {
      "epoch": 1.3417956238832136,
      "grad_norm": 10.741158485412598,
      "learning_rate": 1.1066606660666068e-05,
      "loss": 0.584,
      "step": 19900
    },
    {
      "epoch": 1.345167054381174,
      "grad_norm": 6.761290550231934,
      "learning_rate": 1.1044104410441045e-05,
      "loss": 0.5596,
      "step": 19950
    },
    {
      "epoch": 1.3485384848791342,
      "grad_norm": 5.5339789390563965,
      "learning_rate": 1.1021602160216023e-05,
      "loss": 0.5414,
      "step": 20000
    },
    {
      "epoch": 1.3519099153770946,
      "grad_norm": 6.497238636016846,
      "learning_rate": 1.0999099909991e-05,
      "loss": 0.5882,
      "step": 20050
    },
    {
      "epoch": 1.3552813458750548,
      "grad_norm": 7.099852085113525,
      "learning_rate": 1.0976597659765978e-05,
      "loss": 0.5349,
      "step": 20100
    },
    {
      "epoch": 1.358652776373015,
      "grad_norm": 7.463034629821777,
      "learning_rate": 1.0954095409540954e-05,
      "loss": 0.6171,
      "step": 20150
    },
    {
      "epoch": 1.3620242068709754,
      "grad_norm": 17.25542640686035,
      "learning_rate": 1.0931593159315931e-05,
      "loss": 0.5167,
      "step": 20200
    },
    {
      "epoch": 1.3653956373689358,
      "grad_norm": 6.926758766174316,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 0.5429,
      "step": 20250
    },
    {
      "epoch": 1.368767067866896,
      "grad_norm": 5.705153942108154,
      "learning_rate": 1.0886588658865886e-05,
      "loss": 0.5913,
      "step": 20300
    },
    {
      "epoch": 1.3721384983648561,
      "grad_norm": 6.985871315002441,
      "learning_rate": 1.0864086408640866e-05,
      "loss": 0.5676,
      "step": 20350
    },
    {
      "epoch": 1.3755099288628165,
      "grad_norm": 4.5084967613220215,
      "learning_rate": 1.0841584158415843e-05,
      "loss": 0.5694,
      "step": 20400
    },
    {
      "epoch": 1.3788813593607767,
      "grad_norm": 4.8123884201049805,
      "learning_rate": 1.081908190819082e-05,
      "loss": 0.4873,
      "step": 20450
    },
    {
      "epoch": 1.3822527898587371,
      "grad_norm": 5.401590347290039,
      "learning_rate": 1.0796579657965798e-05,
      "loss": 0.5743,
      "step": 20500
    },
    {
      "epoch": 1.3856242203566973,
      "grad_norm": 5.435759544372559,
      "learning_rate": 1.0774077407740776e-05,
      "loss": 0.5354,
      "step": 20550
    },
    {
      "epoch": 1.3889956508546577,
      "grad_norm": 8.325545310974121,
      "learning_rate": 1.0751575157515752e-05,
      "loss": 0.5539,
      "step": 20600
    },
    {
      "epoch": 1.3923670813526179,
      "grad_norm": 10.456347465515137,
      "learning_rate": 1.072907290729073e-05,
      "loss": 0.5363,
      "step": 20650
    },
    {
      "epoch": 1.395738511850578,
      "grad_norm": 8.731287956237793,
      "learning_rate": 1.0706570657065707e-05,
      "loss": 0.5645,
      "step": 20700
    },
    {
      "epoch": 1.3991099423485385,
      "grad_norm": 11.447600364685059,
      "learning_rate": 1.0684068406840684e-05,
      "loss": 0.5703,
      "step": 20750
    },
    {
      "epoch": 1.4024813728464989,
      "grad_norm": 7.522599220275879,
      "learning_rate": 1.0661566156615662e-05,
      "loss": 0.5892,
      "step": 20800
    },
    {
      "epoch": 1.405852803344459,
      "grad_norm": 5.3264479637146,
      "learning_rate": 1.0639063906390641e-05,
      "loss": 0.5619,
      "step": 20850
    },
    {
      "epoch": 1.4092242338424192,
      "grad_norm": 8.707012176513672,
      "learning_rate": 1.0616561656165619e-05,
      "loss": 0.5022,
      "step": 20900
    },
    {
      "epoch": 1.4125956643403796,
      "grad_norm": 8.806764602661133,
      "learning_rate": 1.0594059405940596e-05,
      "loss": 0.5785,
      "step": 20950
    },
    {
      "epoch": 1.41596709483834,
      "grad_norm": 7.882150173187256,
      "learning_rate": 1.0571557155715572e-05,
      "loss": 0.5541,
      "step": 21000
    },
    {
      "epoch": 1.4193385253363002,
      "grad_norm": 7.900424957275391,
      "learning_rate": 1.054905490549055e-05,
      "loss": 0.4933,
      "step": 21050
    },
    {
      "epoch": 1.4227099558342604,
      "grad_norm": 12.895586967468262,
      "learning_rate": 1.0526552655265527e-05,
      "loss": 0.5648,
      "step": 21100
    },
    {
      "epoch": 1.4260813863322208,
      "grad_norm": 5.8450751304626465,
      "learning_rate": 1.0504050405040505e-05,
      "loss": 0.5739,
      "step": 21150
    },
    {
      "epoch": 1.429452816830181,
      "grad_norm": 12.317397117614746,
      "learning_rate": 1.0481548154815482e-05,
      "loss": 0.5714,
      "step": 21200
    },
    {
      "epoch": 1.4328242473281414,
      "grad_norm": 9.513345718383789,
      "learning_rate": 1.045904590459046e-05,
      "loss": 0.5489,
      "step": 21250
    },
    {
      "epoch": 1.4361956778261016,
      "grad_norm": 12.06959342956543,
      "learning_rate": 1.0436543654365436e-05,
      "loss": 0.5404,
      "step": 21300
    },
    {
      "epoch": 1.439567108324062,
      "grad_norm": 7.964427471160889,
      "learning_rate": 1.0414041404140416e-05,
      "loss": 0.5055,
      "step": 21350
    },
    {
      "epoch": 1.4429385388220222,
      "grad_norm": 8.894289016723633,
      "learning_rate": 1.0391539153915392e-05,
      "loss": 0.5371,
      "step": 21400
    },
    {
      "epoch": 1.4463099693199823,
      "grad_norm": 9.692209243774414,
      "learning_rate": 1.036903690369037e-05,
      "loss": 0.5501,
      "step": 21450
    },
    {
      "epoch": 1.4496813998179428,
      "grad_norm": 6.296205520629883,
      "learning_rate": 1.0346534653465347e-05,
      "loss": 0.5569,
      "step": 21500
    },
    {
      "epoch": 1.4530528303159032,
      "grad_norm": 12.510269165039062,
      "learning_rate": 1.0324032403240325e-05,
      "loss": 0.5682,
      "step": 21550
    },
    {
      "epoch": 1.4564242608138633,
      "grad_norm": 9.660386085510254,
      "learning_rate": 1.0301530153015302e-05,
      "loss": 0.5648,
      "step": 21600
    },
    {
      "epoch": 1.4597956913118235,
      "grad_norm": 9.413920402526855,
      "learning_rate": 1.027902790279028e-05,
      "loss": 0.4911,
      "step": 21650
    },
    {
      "epoch": 1.463167121809784,
      "grad_norm": 8.848851203918457,
      "learning_rate": 1.0256525652565258e-05,
      "loss": 0.5815,
      "step": 21700
    },
    {
      "epoch": 1.466538552307744,
      "grad_norm": 8.565113067626953,
      "learning_rate": 1.0234023402340233e-05,
      "loss": 0.5262,
      "step": 21750
    },
    {
      "epoch": 1.4699099828057045,
      "grad_norm": 8.969926834106445,
      "learning_rate": 1.0211521152115211e-05,
      "loss": 0.5489,
      "step": 21800
    },
    {
      "epoch": 1.4732814133036647,
      "grad_norm": 10.350994110107422,
      "learning_rate": 1.018901890189019e-05,
      "loss": 0.5295,
      "step": 21850
    },
    {
      "epoch": 1.476652843801625,
      "grad_norm": 6.800142288208008,
      "learning_rate": 1.0166516651665168e-05,
      "loss": 0.5698,
      "step": 21900
    },
    {
      "epoch": 1.4800242742995853,
      "grad_norm": 8.241384506225586,
      "learning_rate": 1.0144014401440145e-05,
      "loss": 0.5787,
      "step": 21950
    },
    {
      "epoch": 1.4833957047975457,
      "grad_norm": 10.073868751525879,
      "learning_rate": 1.0121512151215123e-05,
      "loss": 0.5344,
      "step": 22000
    },
    {
      "epoch": 1.4867671352955059,
      "grad_norm": 7.894560813903809,
      "learning_rate": 1.00990099009901e-05,
      "loss": 0.5877,
      "step": 22050
    },
    {
      "epoch": 1.4901385657934663,
      "grad_norm": 5.986392021179199,
      "learning_rate": 1.0076507650765078e-05,
      "loss": 0.5083,
      "step": 22100
    },
    {
      "epoch": 1.4935099962914264,
      "grad_norm": 7.606240272521973,
      "learning_rate": 1.0054005400540054e-05,
      "loss": 0.5615,
      "step": 22150
    },
    {
      "epoch": 1.4968814267893866,
      "grad_norm": 12.01883316040039,
      "learning_rate": 1.0031503150315031e-05,
      "loss": 0.5519,
      "step": 22200
    },
    {
      "epoch": 1.500252857287347,
      "grad_norm": 8.825370788574219,
      "learning_rate": 1.0009000900090009e-05,
      "loss": 0.5118,
      "step": 22250
    },
    {
      "epoch": 1.5036242877853074,
      "grad_norm": 8.02746295928955,
      "learning_rate": 9.986498649864988e-06,
      "loss": 0.539,
      "step": 22300
    },
    {
      "epoch": 1.5069957182832676,
      "grad_norm": 7.592337608337402,
      "learning_rate": 9.963996399639964e-06,
      "loss": 0.6502,
      "step": 22350
    },
    {
      "epoch": 1.5103671487812278,
      "grad_norm": 7.4877028465271,
      "learning_rate": 9.941494149414941e-06,
      "loss": 0.5752,
      "step": 22400
    },
    {
      "epoch": 1.5137385792791882,
      "grad_norm": 5.97702693939209,
      "learning_rate": 9.91899189918992e-06,
      "loss": 0.5522,
      "step": 22450
    },
    {
      "epoch": 1.5171100097771486,
      "grad_norm": 15.37395191192627,
      "learning_rate": 9.896489648964898e-06,
      "loss": 0.5672,
      "step": 22500
    },
    {
      "epoch": 1.5204814402751086,
      "grad_norm": 9.429597854614258,
      "learning_rate": 9.873987398739876e-06,
      "loss": 0.5492,
      "step": 22550
    },
    {
      "epoch": 1.523852870773069,
      "grad_norm": 6.949679851531982,
      "learning_rate": 9.851485148514852e-06,
      "loss": 0.5568,
      "step": 22600
    },
    {
      "epoch": 1.5272243012710294,
      "grad_norm": 11.67339038848877,
      "learning_rate": 9.828982898289829e-06,
      "loss": 0.5269,
      "step": 22650
    },
    {
      "epoch": 1.5305957317689896,
      "grad_norm": 15.857236862182617,
      "learning_rate": 9.806480648064808e-06,
      "loss": 0.5577,
      "step": 22700
    },
    {
      "epoch": 1.5339671622669497,
      "grad_norm": 8.008747100830078,
      "learning_rate": 9.783978397839786e-06,
      "loss": 0.5484,
      "step": 22750
    },
    {
      "epoch": 1.5373385927649101,
      "grad_norm": 11.202341079711914,
      "learning_rate": 9.761476147614762e-06,
      "loss": 0.5118,
      "step": 22800
    },
    {
      "epoch": 1.5407100232628705,
      "grad_norm": 7.317779541015625,
      "learning_rate": 9.73897389738974e-06,
      "loss": 0.5861,
      "step": 22850
    },
    {
      "epoch": 1.5440814537608307,
      "grad_norm": 9.766314506530762,
      "learning_rate": 9.716471647164717e-06,
      "loss": 0.5641,
      "step": 22900
    },
    {
      "epoch": 1.547452884258791,
      "grad_norm": 5.159240245819092,
      "learning_rate": 9.693969396939696e-06,
      "loss": 0.5066,
      "step": 22950
    },
    {
      "epoch": 1.5508243147567513,
      "grad_norm": 8.301228523254395,
      "learning_rate": 9.671467146714672e-06,
      "loss": 0.5681,
      "step": 23000
    },
    {
      "epoch": 1.5541957452547117,
      "grad_norm": 7.486522197723389,
      "learning_rate": 9.64896489648965e-06,
      "loss": 0.5606,
      "step": 23050
    },
    {
      "epoch": 1.557567175752672,
      "grad_norm": 7.582804203033447,
      "learning_rate": 9.626462646264627e-06,
      "loss": 0.4889,
      "step": 23100
    },
    {
      "epoch": 1.560938606250632,
      "grad_norm": 6.867936611175537,
      "learning_rate": 9.603960396039604e-06,
      "loss": 0.5833,
      "step": 23150
    },
    {
      "epoch": 1.5643100367485925,
      "grad_norm": 8.123642921447754,
      "learning_rate": 9.581458145814582e-06,
      "loss": 0.5441,
      "step": 23200
    },
    {
      "epoch": 1.5676814672465527,
      "grad_norm": 11.592260360717773,
      "learning_rate": 9.55895589558956e-06,
      "loss": 0.5291,
      "step": 23250
    },
    {
      "epoch": 1.5710528977445128,
      "grad_norm": 12.958434104919434,
      "learning_rate": 9.536453645364537e-06,
      "loss": 0.5773,
      "step": 23300
    },
    {
      "epoch": 1.5744243282424732,
      "grad_norm": 9.138470649719238,
      "learning_rate": 9.513951395139515e-06,
      "loss": 0.5713,
      "step": 23350
    },
    {
      "epoch": 1.5777957587404337,
      "grad_norm": 7.742380619049072,
      "learning_rate": 9.491449144914492e-06,
      "loss": 0.5683,
      "step": 23400
    },
    {
      "epoch": 1.5811671892383938,
      "grad_norm": 10.961782455444336,
      "learning_rate": 9.46894689468947e-06,
      "loss": 0.5918,
      "step": 23450
    },
    {
      "epoch": 1.584538619736354,
      "grad_norm": 8.320099830627441,
      "learning_rate": 9.446444644464447e-06,
      "loss": 0.5514,
      "step": 23500
    },
    {
      "epoch": 1.5879100502343144,
      "grad_norm": 5.905007362365723,
      "learning_rate": 9.423942394239425e-06,
      "loss": 0.5305,
      "step": 23550
    },
    {
      "epoch": 1.5912814807322748,
      "grad_norm": 9.382638931274414,
      "learning_rate": 9.401440144014402e-06,
      "loss": 0.554,
      "step": 23600
    },
    {
      "epoch": 1.594652911230235,
      "grad_norm": 11.431835174560547,
      "learning_rate": 9.37893789378938e-06,
      "loss": 0.5524,
      "step": 23650
    },
    {
      "epoch": 1.5980243417281952,
      "grad_norm": 12.589988708496094,
      "learning_rate": 9.356435643564357e-06,
      "loss": 0.5614,
      "step": 23700
    },
    {
      "epoch": 1.6013957722261556,
      "grad_norm": 10.487682342529297,
      "learning_rate": 9.333933393339335e-06,
      "loss": 0.5894,
      "step": 23750
    },
    {
      "epoch": 1.604767202724116,
      "grad_norm": 8.97870922088623,
      "learning_rate": 9.311431143114313e-06,
      "loss": 0.5208,
      "step": 23800
    },
    {
      "epoch": 1.6081386332220762,
      "grad_norm": 8.465771675109863,
      "learning_rate": 9.28892889288929e-06,
      "loss": 0.5721,
      "step": 23850
    },
    {
      "epoch": 1.6115100637200364,
      "grad_norm": 9.843424797058105,
      "learning_rate": 9.266426642664268e-06,
      "loss": 0.5526,
      "step": 23900
    },
    {
      "epoch": 1.6148814942179968,
      "grad_norm": 9.812076568603516,
      "learning_rate": 9.243924392439245e-06,
      "loss": 0.5421,
      "step": 23950
    },
    {
      "epoch": 1.618252924715957,
      "grad_norm": 5.1039228439331055,
      "learning_rate": 9.221422142214223e-06,
      "loss": 0.4689,
      "step": 24000
    },
    {
      "epoch": 1.6216243552139171,
      "grad_norm": 8.216487884521484,
      "learning_rate": 9.1989198919892e-06,
      "loss": 0.5249,
      "step": 24050
    },
    {
      "epoch": 1.6249957857118775,
      "grad_norm": 5.742738723754883,
      "learning_rate": 9.176417641764178e-06,
      "loss": 0.5158,
      "step": 24100
    },
    {
      "epoch": 1.628367216209838,
      "grad_norm": 6.478405475616455,
      "learning_rate": 9.153915391539154e-06,
      "loss": 0.5628,
      "step": 24150
    },
    {
      "epoch": 1.6317386467077981,
      "grad_norm": 9.688841819763184,
      "learning_rate": 9.131413141314133e-06,
      "loss": 0.591,
      "step": 24200
    },
    {
      "epoch": 1.6351100772057583,
      "grad_norm": 13.49108600616455,
      "learning_rate": 9.10891089108911e-06,
      "loss": 0.5808,
      "step": 24250
    },
    {
      "epoch": 1.6384815077037187,
      "grad_norm": 7.710671901702881,
      "learning_rate": 9.086408640864088e-06,
      "loss": 0.5064,
      "step": 24300
    },
    {
      "epoch": 1.641852938201679,
      "grad_norm": 12.078989028930664,
      "learning_rate": 9.063906390639064e-06,
      "loss": 0.5749,
      "step": 24350
    },
    {
      "epoch": 1.6452243686996393,
      "grad_norm": 12.365196228027344,
      "learning_rate": 9.041404140414041e-06,
      "loss": 0.5286,
      "step": 24400
    },
    {
      "epoch": 1.6485957991975995,
      "grad_norm": 16.663257598876953,
      "learning_rate": 9.01890189018902e-06,
      "loss": 0.5497,
      "step": 24450
    },
    {
      "epoch": 1.6519672296955599,
      "grad_norm": 11.820472717285156,
      "learning_rate": 8.996399639963998e-06,
      "loss": 0.5284,
      "step": 24500
    },
    {
      "epoch": 1.6553386601935203,
      "grad_norm": 9.678101539611816,
      "learning_rate": 8.973897389738974e-06,
      "loss": 0.5255,
      "step": 24550
    },
    {
      "epoch": 1.6587100906914802,
      "grad_norm": 8.416508674621582,
      "learning_rate": 8.951395139513951e-06,
      "loss": 0.5364,
      "step": 24600
    },
    {
      "epoch": 1.6620815211894406,
      "grad_norm": 8.441969871520996,
      "learning_rate": 8.928892889288929e-06,
      "loss": 0.5132,
      "step": 24650
    },
    {
      "epoch": 1.665452951687401,
      "grad_norm": 10.068954467773438,
      "learning_rate": 8.906390639063908e-06,
      "loss": 0.5396,
      "step": 24700
    },
    {
      "epoch": 1.6688243821853612,
      "grad_norm": 9.125601768493652,
      "learning_rate": 8.883888388838886e-06,
      "loss": 0.5046,
      "step": 24750
    },
    {
      "epoch": 1.6721958126833214,
      "grad_norm": 15.466150283813477,
      "learning_rate": 8.861386138613862e-06,
      "loss": 0.5567,
      "step": 24800
    },
    {
      "epoch": 1.6755672431812818,
      "grad_norm": 9.78710651397705,
      "learning_rate": 8.838883888388839e-06,
      "loss": 0.5271,
      "step": 24850
    },
    {
      "epoch": 1.6789386736792422,
      "grad_norm": 6.671306610107422,
      "learning_rate": 8.816381638163817e-06,
      "loss": 0.5684,
      "step": 24900
    },
    {
      "epoch": 1.6823101041772024,
      "grad_norm": 5.688209056854248,
      "learning_rate": 8.793879387938796e-06,
      "loss": 0.5193,
      "step": 24950
    },
    {
      "epoch": 1.6856815346751626,
      "grad_norm": 10.542949676513672,
      "learning_rate": 8.771377137713772e-06,
      "loss": 0.4844,
      "step": 25000
    },
    {
      "epoch": 1.689052965173123,
      "grad_norm": 8.328598022460938,
      "learning_rate": 8.74887488748875e-06,
      "loss": 0.5241,
      "step": 25050
    },
    {
      "epoch": 1.6924243956710834,
      "grad_norm": 7.519619941711426,
      "learning_rate": 8.726372637263727e-06,
      "loss": 0.5106,
      "step": 25100
    },
    {
      "epoch": 1.6957958261690436,
      "grad_norm": 9.548600196838379,
      "learning_rate": 8.703870387038704e-06,
      "loss": 0.5195,
      "step": 25150
    },
    {
      "epoch": 1.6991672566670037,
      "grad_norm": 9.589509963989258,
      "learning_rate": 8.681368136813682e-06,
      "loss": 0.5394,
      "step": 25200
    },
    {
      "epoch": 1.7025386871649641,
      "grad_norm": 6.970845699310303,
      "learning_rate": 8.65886588658866e-06,
      "loss": 0.5095,
      "step": 25250
    },
    {
      "epoch": 1.7059101176629243,
      "grad_norm": 17.4930362701416,
      "learning_rate": 8.636363636363637e-06,
      "loss": 0.5412,
      "step": 25300
    },
    {
      "epoch": 1.7092815481608845,
      "grad_norm": 10.778401374816895,
      "learning_rate": 8.613861386138615e-06,
      "loss": 0.4865,
      "step": 25350
    },
    {
      "epoch": 1.712652978658845,
      "grad_norm": 10.454005241394043,
      "learning_rate": 8.591359135913592e-06,
      "loss": 0.4909,
      "step": 25400
    },
    {
      "epoch": 1.7160244091568053,
      "grad_norm": 8.218902587890625,
      "learning_rate": 8.56885688568857e-06,
      "loss": 0.5138,
      "step": 25450
    },
    {
      "epoch": 1.7193958396547655,
      "grad_norm": 13.315811157226562,
      "learning_rate": 8.546354635463547e-06,
      "loss": 0.5127,
      "step": 25500
    },
    {
      "epoch": 1.7227672701527257,
      "grad_norm": 8.26288890838623,
      "learning_rate": 8.523852385238525e-06,
      "loss": 0.5372,
      "step": 25550
    },
    {
      "epoch": 1.726138700650686,
      "grad_norm": 17.73724937438965,
      "learning_rate": 8.501350135013502e-06,
      "loss": 0.4787,
      "step": 25600
    },
    {
      "epoch": 1.7295101311486465,
      "grad_norm": 6.444211483001709,
      "learning_rate": 8.47884788478848e-06,
      "loss": 0.5049,
      "step": 25650
    },
    {
      "epoch": 1.7328815616466067,
      "grad_norm": 8.582941055297852,
      "learning_rate": 8.456345634563457e-06,
      "loss": 0.5518,
      "step": 25700
    },
    {
      "epoch": 1.7362529921445669,
      "grad_norm": 8.82585334777832,
      "learning_rate": 8.433843384338435e-06,
      "loss": 0.5415,
      "step": 25750
    },
    {
      "epoch": 1.7396244226425273,
      "grad_norm": 6.4164137840271,
      "learning_rate": 8.411341134113412e-06,
      "loss": 0.5057,
      "step": 25800
    },
    {
      "epoch": 1.7429958531404877,
      "grad_norm": 7.601696491241455,
      "learning_rate": 8.38883888388839e-06,
      "loss": 0.5444,
      "step": 25850
    },
    {
      "epoch": 1.7463672836384478,
      "grad_norm": 13.448284149169922,
      "learning_rate": 8.366336633663367e-06,
      "loss": 0.538,
      "step": 25900
    },
    {
      "epoch": 1.749738714136408,
      "grad_norm": 8.20482063293457,
      "learning_rate": 8.343834383438345e-06,
      "loss": 0.5249,
      "step": 25950
    },
    {
      "epoch": 1.7531101446343684,
      "grad_norm": 5.0411200523376465,
      "learning_rate": 8.321332133213323e-06,
      "loss": 0.5494,
      "step": 26000
    },
    {
      "epoch": 1.7564815751323286,
      "grad_norm": 9.730578422546387,
      "learning_rate": 8.2988298829883e-06,
      "loss": 0.4966,
      "step": 26050
    },
    {
      "epoch": 1.7598530056302888,
      "grad_norm": 9.495433807373047,
      "learning_rate": 8.276327632763278e-06,
      "loss": 0.5129,
      "step": 26100
    },
    {
      "epoch": 1.7632244361282492,
      "grad_norm": 7.731681823730469,
      "learning_rate": 8.253825382538253e-06,
      "loss": 0.5538,
      "step": 26150
    },
    {
      "epoch": 1.7665958666262096,
      "grad_norm": 5.89726448059082,
      "learning_rate": 8.231323132313233e-06,
      "loss": 0.5501,
      "step": 26200
    },
    {
      "epoch": 1.7699672971241698,
      "grad_norm": 9.642563819885254,
      "learning_rate": 8.20882088208821e-06,
      "loss": 0.4998,
      "step": 26250
    },
    {
      "epoch": 1.77333872762213,
      "grad_norm": 7.260543346405029,
      "learning_rate": 8.186318631863188e-06,
      "loss": 0.5261,
      "step": 26300
    },
    {
      "epoch": 1.7767101581200904,
      "grad_norm": 12.871111869812012,
      "learning_rate": 8.163816381638164e-06,
      "loss": 0.5138,
      "step": 26350
    },
    {
      "epoch": 1.7800815886180508,
      "grad_norm": 13.043519973754883,
      "learning_rate": 8.141314131413141e-06,
      "loss": 0.5158,
      "step": 26400
    },
    {
      "epoch": 1.783453019116011,
      "grad_norm": 7.506998538970947,
      "learning_rate": 8.11881188118812e-06,
      "loss": 0.5182,
      "step": 26450
    },
    {
      "epoch": 1.7868244496139711,
      "grad_norm": 6.773364067077637,
      "learning_rate": 8.096309630963098e-06,
      "loss": 0.5,
      "step": 26500
    },
    {
      "epoch": 1.7901958801119315,
      "grad_norm": 11.91465950012207,
      "learning_rate": 8.073807380738074e-06,
      "loss": 0.5153,
      "step": 26550
    },
    {
      "epoch": 1.793567310609892,
      "grad_norm": 11.07396125793457,
      "learning_rate": 8.051305130513051e-06,
      "loss": 0.582,
      "step": 26600
    },
    {
      "epoch": 1.796938741107852,
      "grad_norm": 15.918045043945312,
      "learning_rate": 8.028802880288029e-06,
      "loss": 0.5095,
      "step": 26650
    },
    {
      "epoch": 1.8003101716058123,
      "grad_norm": 9.943990707397461,
      "learning_rate": 8.006300630063008e-06,
      "loss": 0.4875,
      "step": 26700
    },
    {
      "epoch": 1.8036816021037727,
      "grad_norm": 6.915251731872559,
      "learning_rate": 7.983798379837984e-06,
      "loss": 0.5314,
      "step": 26750
    },
    {
      "epoch": 1.807053032601733,
      "grad_norm": 9.525324821472168,
      "learning_rate": 7.961296129612961e-06,
      "loss": 0.4615,
      "step": 26800
    },
    {
      "epoch": 1.810424463099693,
      "grad_norm": 8.699624061584473,
      "learning_rate": 7.938793879387939e-06,
      "loss": 0.4951,
      "step": 26850
    },
    {
      "epoch": 1.8137958935976535,
      "grad_norm": 9.418989181518555,
      "learning_rate": 7.916291629162917e-06,
      "loss": 0.4926,
      "step": 26900
    },
    {
      "epoch": 1.8171673240956139,
      "grad_norm": 6.458010196685791,
      "learning_rate": 7.893789378937896e-06,
      "loss": 0.4736,
      "step": 26950
    },
    {
      "epoch": 1.820538754593574,
      "grad_norm": 6.966925621032715,
      "learning_rate": 7.871287128712872e-06,
      "loss": 0.5551,
      "step": 27000
    },
    {
      "epoch": 1.8239101850915342,
      "grad_norm": 8.202085494995117,
      "learning_rate": 7.84878487848785e-06,
      "loss": 0.503,
      "step": 27050
    },
    {
      "epoch": 1.8272816155894946,
      "grad_norm": 7.513713359832764,
      "learning_rate": 7.826282628262827e-06,
      "loss": 0.5538,
      "step": 27100
    },
    {
      "epoch": 1.830653046087455,
      "grad_norm": 9.744908332824707,
      "learning_rate": 7.803780378037804e-06,
      "loss": 0.4726,
      "step": 27150
    },
    {
      "epoch": 1.8340244765854152,
      "grad_norm": 9.478434562683105,
      "learning_rate": 7.781278127812782e-06,
      "loss": 0.5152,
      "step": 27200
    },
    {
      "epoch": 1.8373959070833754,
      "grad_norm": 6.326585292816162,
      "learning_rate": 7.75877587758776e-06,
      "loss": 0.5036,
      "step": 27250
    },
    {
      "epoch": 1.8407673375813358,
      "grad_norm": 12.236090660095215,
      "learning_rate": 7.736273627362737e-06,
      "loss": 0.5469,
      "step": 27300
    },
    {
      "epoch": 1.844138768079296,
      "grad_norm": 8.856123924255371,
      "learning_rate": 7.713771377137714e-06,
      "loss": 0.4905,
      "step": 27350
    },
    {
      "epoch": 1.8475101985772562,
      "grad_norm": 9.97114086151123,
      "learning_rate": 7.691269126912692e-06,
      "loss": 0.5188,
      "step": 27400
    },
    {
      "epoch": 1.8508816290752166,
      "grad_norm": 8.149540901184082,
      "learning_rate": 7.66876687668767e-06,
      "loss": 0.5591,
      "step": 27450
    },
    {
      "epoch": 1.854253059573177,
      "grad_norm": 6.9495463371276855,
      "learning_rate": 7.646264626462647e-06,
      "loss": 0.5646,
      "step": 27500
    },
    {
      "epoch": 1.8576244900711372,
      "grad_norm": 11.68581485748291,
      "learning_rate": 7.6237623762376246e-06,
      "loss": 0.5596,
      "step": 27550
    },
    {
      "epoch": 1.8609959205690974,
      "grad_norm": 9.045252799987793,
      "learning_rate": 7.601260126012602e-06,
      "loss": 0.5146,
      "step": 27600
    },
    {
      "epoch": 1.8643673510670578,
      "grad_norm": 6.695042610168457,
      "learning_rate": 7.578757875787579e-06,
      "loss": 0.4981,
      "step": 27650
    },
    {
      "epoch": 1.8677387815650182,
      "grad_norm": 6.914981842041016,
      "learning_rate": 7.556255625562557e-06,
      "loss": 0.5519,
      "step": 27700
    },
    {
      "epoch": 1.8711102120629783,
      "grad_norm": 6.602471351623535,
      "learning_rate": 7.533753375337535e-06,
      "loss": 0.5974,
      "step": 27750
    },
    {
      "epoch": 1.8744816425609385,
      "grad_norm": 9.148005485534668,
      "learning_rate": 7.511251125112512e-06,
      "loss": 0.5412,
      "step": 27800
    },
    {
      "epoch": 1.877853073058899,
      "grad_norm": 7.565360069274902,
      "learning_rate": 7.488748874887489e-06,
      "loss": 0.4811,
      "step": 27850
    },
    {
      "epoch": 1.8812245035568593,
      "grad_norm": 6.472214698791504,
      "learning_rate": 7.4662466246624665e-06,
      "loss": 0.5185,
      "step": 27900
    },
    {
      "epoch": 1.8845959340548195,
      "grad_norm": 7.354593753814697,
      "learning_rate": 7.443744374437444e-06,
      "loss": 0.5107,
      "step": 27950
    },
    {
      "epoch": 1.8879673645527797,
      "grad_norm": 7.029138565063477,
      "learning_rate": 7.4212421242124224e-06,
      "loss": 0.4946,
      "step": 28000
    },
    {
      "epoch": 1.89133879505074,
      "grad_norm": 11.778909683227539,
      "learning_rate": 7.398739873987399e-06,
      "loss": 0.5088,
      "step": 28050
    },
    {
      "epoch": 1.8947102255487003,
      "grad_norm": 8.95099925994873,
      "learning_rate": 7.376237623762377e-06,
      "loss": 0.5168,
      "step": 28100
    },
    {
      "epoch": 1.8980816560466605,
      "grad_norm": 7.800292015075684,
      "learning_rate": 7.353735373537354e-06,
      "loss": 0.5077,
      "step": 28150
    },
    {
      "epoch": 1.9014530865446209,
      "grad_norm": 8.442886352539062,
      "learning_rate": 7.331233123312331e-06,
      "loss": 0.5233,
      "step": 28200
    },
    {
      "epoch": 1.9048245170425813,
      "grad_norm": 10.27510929107666,
      "learning_rate": 7.308730873087309e-06,
      "loss": 0.4893,
      "step": 28250
    },
    {
      "epoch": 1.9081959475405414,
      "grad_norm": 6.7539544105529785,
      "learning_rate": 7.286228622862287e-06,
      "loss": 0.5121,
      "step": 28300
    },
    {
      "epoch": 1.9115673780385016,
      "grad_norm": 8.729541778564453,
      "learning_rate": 7.263726372637264e-06,
      "loss": 0.4679,
      "step": 28350
    },
    {
      "epoch": 1.914938808536462,
      "grad_norm": 6.551263332366943,
      "learning_rate": 7.241224122412242e-06,
      "loss": 0.4835,
      "step": 28400
    },
    {
      "epoch": 1.9183102390344224,
      "grad_norm": 9.605484962463379,
      "learning_rate": 7.218721872187219e-06,
      "loss": 0.4998,
      "step": 28450
    },
    {
      "epoch": 1.9216816695323826,
      "grad_norm": 10.177495002746582,
      "learning_rate": 7.196219621962197e-06,
      "loss": 0.5223,
      "step": 28500
    },
    {
      "epoch": 1.9250531000303428,
      "grad_norm": 12.695277214050293,
      "learning_rate": 7.1737173717371745e-06,
      "loss": 0.5076,
      "step": 28550
    },
    {
      "epoch": 1.9284245305283032,
      "grad_norm": 6.477205753326416,
      "learning_rate": 7.151215121512152e-06,
      "loss": 0.493,
      "step": 28600
    },
    {
      "epoch": 1.9317959610262636,
      "grad_norm": 7.604761123657227,
      "learning_rate": 7.128712871287129e-06,
      "loss": 0.5019,
      "step": 28650
    },
    {
      "epoch": 1.9351673915242236,
      "grad_norm": 12.522932052612305,
      "learning_rate": 7.106210621062106e-06,
      "loss": 0.5648,
      "step": 28700
    },
    {
      "epoch": 1.938538822022184,
      "grad_norm": 7.633313179016113,
      "learning_rate": 7.083708370837085e-06,
      "loss": 0.5131,
      "step": 28750
    },
    {
      "epoch": 1.9419102525201444,
      "grad_norm": 3.8872947692871094,
      "learning_rate": 7.061206120612062e-06,
      "loss": 0.4886,
      "step": 28800
    },
    {
      "epoch": 1.9452816830181046,
      "grad_norm": 5.889340877532959,
      "learning_rate": 7.038703870387039e-06,
      "loss": 0.4922,
      "step": 28850
    },
    {
      "epoch": 1.9486531135160647,
      "grad_norm": 9.610280990600586,
      "learning_rate": 7.0162016201620164e-06,
      "loss": 0.5125,
      "step": 28900
    },
    {
      "epoch": 1.9520245440140251,
      "grad_norm": 9.141276359558105,
      "learning_rate": 6.993699369936994e-06,
      "loss": 0.4889,
      "step": 28950
    },
    {
      "epoch": 1.9553959745119855,
      "grad_norm": 7.706587314605713,
      "learning_rate": 6.971197119711972e-06,
      "loss": 0.5132,
      "step": 29000
    },
    {
      "epoch": 1.9587674050099457,
      "grad_norm": 11.41386604309082,
      "learning_rate": 6.948694869486949e-06,
      "loss": 0.4937,
      "step": 29050
    },
    {
      "epoch": 1.962138835507906,
      "grad_norm": 11.020838737487793,
      "learning_rate": 6.926192619261927e-06,
      "loss": 0.5244,
      "step": 29100
    },
    {
      "epoch": 1.9655102660058663,
      "grad_norm": 6.5713114738464355,
      "learning_rate": 6.903690369036904e-06,
      "loss": 0.5205,
      "step": 29150
    },
    {
      "epoch": 1.9688816965038267,
      "grad_norm": 5.534255504608154,
      "learning_rate": 6.881188118811881e-06,
      "loss": 0.4988,
      "step": 29200
    },
    {
      "epoch": 1.972253127001787,
      "grad_norm": 7.652472496032715,
      "learning_rate": 6.858685868586859e-06,
      "loss": 0.5074,
      "step": 29250
    },
    {
      "epoch": 1.975624557499747,
      "grad_norm": 8.918914794921875,
      "learning_rate": 6.836183618361837e-06,
      "loss": 0.4616,
      "step": 29300
    },
    {
      "epoch": 1.9789959879977075,
      "grad_norm": 10.052921295166016,
      "learning_rate": 6.813681368136814e-06,
      "loss": 0.5309,
      "step": 29350
    },
    {
      "epoch": 1.9823674184956677,
      "grad_norm": 7.720654487609863,
      "learning_rate": 6.791179117911791e-06,
      "loss": 0.4916,
      "step": 29400
    },
    {
      "epoch": 1.9857388489936278,
      "grad_norm": 7.642268180847168,
      "learning_rate": 6.7686768676867685e-06,
      "loss": 0.5343,
      "step": 29450
    },
    {
      "epoch": 1.9891102794915883,
      "grad_norm": 11.68378734588623,
      "learning_rate": 6.746174617461747e-06,
      "loss": 0.4658,
      "step": 29500
    },
    {
      "epoch": 1.9924817099895487,
      "grad_norm": 6.757956504821777,
      "learning_rate": 6.7236723672367245e-06,
      "loss": 0.5299,
      "step": 29550
    },
    {
      "epoch": 1.9958531404875088,
      "grad_norm": 8.8197660446167,
      "learning_rate": 6.701170117011702e-06,
      "loss": 0.5103,
      "step": 29600
    },
    {
      "epoch": 1.999224570985469,
      "grad_norm": 7.477020263671875,
      "learning_rate": 6.678667866786679e-06,
      "loss": 0.5103,
      "step": 29650
    },
    {
      "epoch": 2.0025622871784496,
      "grad_norm": 8.525897026062012,
      "learning_rate": 6.656165616561656e-06,
      "loss": 0.4541,
      "step": 29700
    },
    {
      "epoch": 2.00593371767641,
      "grad_norm": 10.58384895324707,
      "learning_rate": 6.633663366336635e-06,
      "loss": 0.4499,
      "step": 29750
    },
    {
      "epoch": 2.0093051481743704,
      "grad_norm": 6.2572150230407715,
      "learning_rate": 6.611161116111612e-06,
      "loss": 0.4947,
      "step": 29800
    },
    {
      "epoch": 2.012676578672331,
      "grad_norm": 9.018559455871582,
      "learning_rate": 6.588658865886589e-06,
      "loss": 0.4827,
      "step": 29850
    },
    {
      "epoch": 2.016048009170291,
      "grad_norm": 9.880606651306152,
      "learning_rate": 6.566156615661566e-06,
      "loss": 0.47,
      "step": 29900
    },
    {
      "epoch": 2.019419439668251,
      "grad_norm": 7.985138416290283,
      "learning_rate": 6.543654365436544e-06,
      "loss": 0.5235,
      "step": 29950
    },
    {
      "epoch": 2.0227908701662116,
      "grad_norm": 7.9042181968688965,
      "learning_rate": 6.521152115211522e-06,
      "loss": 0.4256,
      "step": 30000
    },
    {
      "epoch": 2.026162300664172,
      "grad_norm": 4.284524917602539,
      "learning_rate": 6.498649864986499e-06,
      "loss": 0.4751,
      "step": 30050
    },
    {
      "epoch": 2.029533731162132,
      "grad_norm": 9.4891357421875,
      "learning_rate": 6.4761476147614765e-06,
      "loss": 0.4561,
      "step": 30100
    },
    {
      "epoch": 2.0329051616600924,
      "grad_norm": 7.8592448234558105,
      "learning_rate": 6.453645364536454e-06,
      "loss": 0.5395,
      "step": 30150
    },
    {
      "epoch": 2.036276592158053,
      "grad_norm": 7.619889259338379,
      "learning_rate": 6.431143114311431e-06,
      "loss": 0.541,
      "step": 30200
    },
    {
      "epoch": 2.0396480226560127,
      "grad_norm": 5.629939556121826,
      "learning_rate": 6.408640864086409e-06,
      "loss": 0.5254,
      "step": 30250
    },
    {
      "epoch": 2.043019453153973,
      "grad_norm": 8.233341217041016,
      "learning_rate": 6.386138613861387e-06,
      "loss": 0.5067,
      "step": 30300
    },
    {
      "epoch": 2.0463908836519336,
      "grad_norm": 9.387527465820312,
      "learning_rate": 6.363636363636364e-06,
      "loss": 0.4883,
      "step": 30350
    },
    {
      "epoch": 2.049762314149894,
      "grad_norm": 7.088895797729492,
      "learning_rate": 6.341134113411341e-06,
      "loss": 0.4554,
      "step": 30400
    },
    {
      "epoch": 2.053133744647854,
      "grad_norm": 6.132421016693115,
      "learning_rate": 6.3186318631863185e-06,
      "loss": 0.5153,
      "step": 30450
    },
    {
      "epoch": 2.0565051751458143,
      "grad_norm": 11.156042098999023,
      "learning_rate": 6.296129612961297e-06,
      "loss": 0.4587,
      "step": 30500
    },
    {
      "epoch": 2.0598766056437747,
      "grad_norm": 8.500505447387695,
      "learning_rate": 6.273627362736274e-06,
      "loss": 0.4444,
      "step": 30550
    },
    {
      "epoch": 2.063248036141735,
      "grad_norm": 8.39848518371582,
      "learning_rate": 6.251125112511252e-06,
      "loss": 0.4558,
      "step": 30600
    },
    {
      "epoch": 2.066619466639695,
      "grad_norm": 11.257896423339844,
      "learning_rate": 6.228622862286229e-06,
      "loss": 0.471,
      "step": 30650
    },
    {
      "epoch": 2.0699908971376555,
      "grad_norm": 8.836081504821777,
      "learning_rate": 6.206120612061206e-06,
      "loss": 0.4756,
      "step": 30700
    },
    {
      "epoch": 2.073362327635616,
      "grad_norm": 11.336677551269531,
      "learning_rate": 6.1836183618361845e-06,
      "loss": 0.5242,
      "step": 30750
    },
    {
      "epoch": 2.0767337581335763,
      "grad_norm": 8.348974227905273,
      "learning_rate": 6.161116111611162e-06,
      "loss": 0.4818,
      "step": 30800
    },
    {
      "epoch": 2.0801051886315363,
      "grad_norm": 6.388078689575195,
      "learning_rate": 6.138613861386139e-06,
      "loss": 0.5144,
      "step": 30850
    },
    {
      "epoch": 2.0834766191294967,
      "grad_norm": 12.844036102294922,
      "learning_rate": 6.116111611161116e-06,
      "loss": 0.472,
      "step": 30900
    },
    {
      "epoch": 2.086848049627457,
      "grad_norm": 8.249031066894531,
      "learning_rate": 6.093609360936094e-06,
      "loss": 0.4735,
      "step": 30950
    },
    {
      "epoch": 2.090219480125417,
      "grad_norm": 6.26026725769043,
      "learning_rate": 6.071107110711072e-06,
      "loss": 0.4585,
      "step": 31000
    },
    {
      "epoch": 2.0935909106233774,
      "grad_norm": 6.2097649574279785,
      "learning_rate": 6.048604860486049e-06,
      "loss": 0.5065,
      "step": 31050
    },
    {
      "epoch": 2.096962341121338,
      "grad_norm": 14.38469123840332,
      "learning_rate": 6.0261026102610265e-06,
      "loss": 0.517,
      "step": 31100
    },
    {
      "epoch": 2.1003337716192982,
      "grad_norm": 6.248662948608398,
      "learning_rate": 6.003600360036004e-06,
      "loss": 0.4853,
      "step": 31150
    },
    {
      "epoch": 2.103705202117258,
      "grad_norm": 8.315414428710938,
      "learning_rate": 5.981098109810981e-06,
      "loss": 0.4772,
      "step": 31200
    },
    {
      "epoch": 2.1070766326152186,
      "grad_norm": 7.785849094390869,
      "learning_rate": 5.958595859585959e-06,
      "loss": 0.4764,
      "step": 31250
    },
    {
      "epoch": 2.110448063113179,
      "grad_norm": 7.220902442932129,
      "learning_rate": 5.936093609360937e-06,
      "loss": 0.467,
      "step": 31300
    },
    {
      "epoch": 2.1138194936111394,
      "grad_norm": 9.352790832519531,
      "learning_rate": 5.913591359135914e-06,
      "loss": 0.4894,
      "step": 31350
    },
    {
      "epoch": 2.1171909241090994,
      "grad_norm": 7.2114739418029785,
      "learning_rate": 5.891089108910891e-06,
      "loss": 0.4563,
      "step": 31400
    },
    {
      "epoch": 2.1205623546070598,
      "grad_norm": 5.152011871337891,
      "learning_rate": 5.868586858685868e-06,
      "loss": 0.501,
      "step": 31450
    },
    {
      "epoch": 2.12393378510502,
      "grad_norm": 7.692129611968994,
      "learning_rate": 5.846084608460847e-06,
      "loss": 0.4538,
      "step": 31500
    },
    {
      "epoch": 2.12730521560298,
      "grad_norm": 7.006762981414795,
      "learning_rate": 5.823582358235824e-06,
      "loss": 0.4654,
      "step": 31550
    },
    {
      "epoch": 2.1306766461009405,
      "grad_norm": 6.6265435218811035,
      "learning_rate": 5.801080108010802e-06,
      "loss": 0.4978,
      "step": 31600
    },
    {
      "epoch": 2.134048076598901,
      "grad_norm": 7.138903617858887,
      "learning_rate": 5.7785778577857786e-06,
      "loss": 0.4934,
      "step": 31650
    },
    {
      "epoch": 2.1374195070968613,
      "grad_norm": 6.203624248504639,
      "learning_rate": 5.756075607560756e-06,
      "loss": 0.4494,
      "step": 31700
    },
    {
      "epoch": 2.1407909375948213,
      "grad_norm": 5.832126617431641,
      "learning_rate": 5.7335733573357345e-06,
      "loss": 0.4869,
      "step": 31750
    },
    {
      "epoch": 2.1441623680927817,
      "grad_norm": 9.102606773376465,
      "learning_rate": 5.711071107110712e-06,
      "loss": 0.4925,
      "step": 31800
    },
    {
      "epoch": 2.147533798590742,
      "grad_norm": 8.591099739074707,
      "learning_rate": 5.688568856885689e-06,
      "loss": 0.4629,
      "step": 31850
    },
    {
      "epoch": 2.1509052290887025,
      "grad_norm": 7.6798930168151855,
      "learning_rate": 5.666066606660666e-06,
      "loss": 0.4847,
      "step": 31900
    },
    {
      "epoch": 2.1542766595866625,
      "grad_norm": 7.495011329650879,
      "learning_rate": 5.643564356435644e-06,
      "loss": 0.4871,
      "step": 31950
    },
    {
      "epoch": 2.157648090084623,
      "grad_norm": 7.630239009857178,
      "learning_rate": 5.621062106210622e-06,
      "loss": 0.4684,
      "step": 32000
    },
    {
      "epoch": 2.1610195205825833,
      "grad_norm": 7.758181571960449,
      "learning_rate": 5.598559855985599e-06,
      "loss": 0.4725,
      "step": 32050
    },
    {
      "epoch": 2.1643909510805432,
      "grad_norm": 11.816556930541992,
      "learning_rate": 5.576057605760576e-06,
      "loss": 0.4623,
      "step": 32100
    },
    {
      "epoch": 2.1677623815785036,
      "grad_norm": 8.189040184020996,
      "learning_rate": 5.553555355535554e-06,
      "loss": 0.5035,
      "step": 32150
    },
    {
      "epoch": 2.171133812076464,
      "grad_norm": 5.577736854553223,
      "learning_rate": 5.531053105310531e-06,
      "loss": 0.4663,
      "step": 32200
    },
    {
      "epoch": 2.1745052425744245,
      "grad_norm": 4.915928363800049,
      "learning_rate": 5.508550855085509e-06,
      "loss": 0.5096,
      "step": 32250
    },
    {
      "epoch": 2.1778766730723844,
      "grad_norm": 8.89174747467041,
      "learning_rate": 5.4860486048604866e-06,
      "loss": 0.4901,
      "step": 32300
    },
    {
      "epoch": 2.181248103570345,
      "grad_norm": 8.312543869018555,
      "learning_rate": 5.463546354635464e-06,
      "loss": 0.476,
      "step": 32350
    },
    {
      "epoch": 2.1846195340683052,
      "grad_norm": 6.507904052734375,
      "learning_rate": 5.441044104410441e-06,
      "loss": 0.4882,
      "step": 32400
    },
    {
      "epoch": 2.1879909645662656,
      "grad_norm": 7.605358600616455,
      "learning_rate": 5.418541854185418e-06,
      "loss": 0.4771,
      "step": 32450
    },
    {
      "epoch": 2.1913623950642256,
      "grad_norm": 9.096762657165527,
      "learning_rate": 5.396039603960397e-06,
      "loss": 0.4776,
      "step": 32500
    },
    {
      "epoch": 2.194733825562186,
      "grad_norm": 4.772045612335205,
      "learning_rate": 5.373537353735374e-06,
      "loss": 0.4874,
      "step": 32550
    },
    {
      "epoch": 2.1981052560601464,
      "grad_norm": 6.728142738342285,
      "learning_rate": 5.351035103510351e-06,
      "loss": 0.4482,
      "step": 32600
    },
    {
      "epoch": 2.201476686558107,
      "grad_norm": 12.058939933776855,
      "learning_rate": 5.3285328532853285e-06,
      "loss": 0.4988,
      "step": 32650
    },
    {
      "epoch": 2.2048481170560668,
      "grad_norm": 7.097023963928223,
      "learning_rate": 5.306030603060306e-06,
      "loss": 0.4457,
      "step": 32700
    },
    {
      "epoch": 2.208219547554027,
      "grad_norm": 4.993061542510986,
      "learning_rate": 5.2835283528352844e-06,
      "loss": 0.4906,
      "step": 32750
    },
    {
      "epoch": 2.2115909780519876,
      "grad_norm": 5.129383563995361,
      "learning_rate": 5.261026102610262e-06,
      "loss": 0.4728,
      "step": 32800
    },
    {
      "epoch": 2.2149624085499475,
      "grad_norm": 9.477270126342773,
      "learning_rate": 5.238523852385239e-06,
      "loss": 0.4472,
      "step": 32850
    },
    {
      "epoch": 2.218333839047908,
      "grad_norm": 5.955560684204102,
      "learning_rate": 5.216021602160216e-06,
      "loss": 0.4931,
      "step": 32900
    },
    {
      "epoch": 2.2217052695458683,
      "grad_norm": 10.391745567321777,
      "learning_rate": 5.193519351935194e-06,
      "loss": 0.4986,
      "step": 32950
    },
    {
      "epoch": 2.2250767000438287,
      "grad_norm": 7.941402435302734,
      "learning_rate": 5.171017101710172e-06,
      "loss": 0.4636,
      "step": 33000
    },
    {
      "epoch": 2.2284481305417887,
      "grad_norm": 8.877813339233398,
      "learning_rate": 5.148514851485149e-06,
      "loss": 0.4515,
      "step": 33050
    },
    {
      "epoch": 2.231819561039749,
      "grad_norm": 7.400084018707275,
      "learning_rate": 5.126012601260126e-06,
      "loss": 0.4696,
      "step": 33100
    },
    {
      "epoch": 2.2351909915377095,
      "grad_norm": 6.390016078948975,
      "learning_rate": 5.103510351035104e-06,
      "loss": 0.479,
      "step": 33150
    },
    {
      "epoch": 2.23856242203567,
      "grad_norm": 6.783498287200928,
      "learning_rate": 5.081008100810081e-06,
      "loss": 0.4453,
      "step": 33200
    },
    {
      "epoch": 2.24193385253363,
      "grad_norm": 8.649394035339355,
      "learning_rate": 5.058505850585059e-06,
      "loss": 0.4747,
      "step": 33250
    },
    {
      "epoch": 2.2453052830315903,
      "grad_norm": 9.830406188964844,
      "learning_rate": 5.0360036003600365e-06,
      "loss": 0.4187,
      "step": 33300
    },
    {
      "epoch": 2.2486767135295507,
      "grad_norm": 6.07955265045166,
      "learning_rate": 5.013501350135014e-06,
      "loss": 0.4508,
      "step": 33350
    },
    {
      "epoch": 2.252048144027511,
      "grad_norm": 5.180375576019287,
      "learning_rate": 4.990999099909992e-06,
      "loss": 0.4408,
      "step": 33400
    },
    {
      "epoch": 2.255419574525471,
      "grad_norm": 6.692272663116455,
      "learning_rate": 4.968496849684969e-06,
      "loss": 0.4977,
      "step": 33450
    },
    {
      "epoch": 2.2587910050234314,
      "grad_norm": 8.512224197387695,
      "learning_rate": 4.945994599459946e-06,
      "loss": 0.4691,
      "step": 33500
    },
    {
      "epoch": 2.262162435521392,
      "grad_norm": 10.931068420410156,
      "learning_rate": 4.923492349234924e-06,
      "loss": 0.4325,
      "step": 33550
    },
    {
      "epoch": 2.265533866019352,
      "grad_norm": 6.811948299407959,
      "learning_rate": 4.900990099009901e-06,
      "loss": 0.5015,
      "step": 33600
    },
    {
      "epoch": 2.268905296517312,
      "grad_norm": 6.050765037536621,
      "learning_rate": 4.878487848784879e-06,
      "loss": 0.4286,
      "step": 33650
    },
    {
      "epoch": 2.2722767270152726,
      "grad_norm": 10.08346939086914,
      "learning_rate": 4.855985598559856e-06,
      "loss": 0.4898,
      "step": 33700
    },
    {
      "epoch": 2.275648157513233,
      "grad_norm": 10.87924575805664,
      "learning_rate": 4.8334833483348335e-06,
      "loss": 0.4792,
      "step": 33750
    },
    {
      "epoch": 2.279019588011193,
      "grad_norm": 6.966372966766357,
      "learning_rate": 4.810981098109812e-06,
      "loss": 0.4718,
      "step": 33800
    },
    {
      "epoch": 2.2823910185091534,
      "grad_norm": 7.432961463928223,
      "learning_rate": 4.788478847884789e-06,
      "loss": 0.4742,
      "step": 33850
    },
    {
      "epoch": 2.285762449007114,
      "grad_norm": 11.018874168395996,
      "learning_rate": 4.765976597659767e-06,
      "loss": 0.435,
      "step": 33900
    },
    {
      "epoch": 2.289133879505074,
      "grad_norm": 5.396212577819824,
      "learning_rate": 4.743474347434744e-06,
      "loss": 0.5129,
      "step": 33950
    },
    {
      "epoch": 2.292505310003034,
      "grad_norm": 9.042998313903809,
      "learning_rate": 4.720972097209721e-06,
      "loss": 0.4323,
      "step": 34000
    },
    {
      "epoch": 2.2958767405009946,
      "grad_norm": 6.967334270477295,
      "learning_rate": 4.698469846984699e-06,
      "loss": 0.4805,
      "step": 34050
    },
    {
      "epoch": 2.299248170998955,
      "grad_norm": 4.189820766448975,
      "learning_rate": 4.675967596759676e-06,
      "loss": 0.4703,
      "step": 34100
    },
    {
      "epoch": 2.3026196014969154,
      "grad_norm": 5.063335418701172,
      "learning_rate": 4.653465346534654e-06,
      "loss": 0.4746,
      "step": 34150
    },
    {
      "epoch": 2.3059910319948753,
      "grad_norm": 5.682431221008301,
      "learning_rate": 4.630963096309631e-06,
      "loss": 0.4546,
      "step": 34200
    },
    {
      "epoch": 2.3093624624928357,
      "grad_norm": 4.304883003234863,
      "learning_rate": 4.608460846084609e-06,
      "loss": 0.4608,
      "step": 34250
    },
    {
      "epoch": 2.312733892990796,
      "grad_norm": 9.708205223083496,
      "learning_rate": 4.5859585958595865e-06,
      "loss": 0.5021,
      "step": 34300
    },
    {
      "epoch": 2.316105323488756,
      "grad_norm": 9.164374351501465,
      "learning_rate": 4.563456345634564e-06,
      "loss": 0.4609,
      "step": 34350
    },
    {
      "epoch": 2.3194767539867165,
      "grad_norm": 8.372598648071289,
      "learning_rate": 4.5409540954095415e-06,
      "loss": 0.4943,
      "step": 34400
    },
    {
      "epoch": 2.322848184484677,
      "grad_norm": 4.389008522033691,
      "learning_rate": 4.518451845184519e-06,
      "loss": 0.4974,
      "step": 34450
    },
    {
      "epoch": 2.3262196149826373,
      "grad_norm": 14.526551246643066,
      "learning_rate": 4.495949594959496e-06,
      "loss": 0.4874,
      "step": 34500
    },
    {
      "epoch": 2.3295910454805973,
      "grad_norm": 10.198019981384277,
      "learning_rate": 4.473447344734474e-06,
      "loss": 0.4605,
      "step": 34550
    },
    {
      "epoch": 2.3329624759785577,
      "grad_norm": 8.290626525878906,
      "learning_rate": 4.450945094509451e-06,
      "loss": 0.4708,
      "step": 34600
    },
    {
      "epoch": 2.336333906476518,
      "grad_norm": 8.757467269897461,
      "learning_rate": 4.428442844284429e-06,
      "loss": 0.4618,
      "step": 34650
    },
    {
      "epoch": 2.339705336974478,
      "grad_norm": 8.209720611572266,
      "learning_rate": 4.405940594059406e-06,
      "loss": 0.4273,
      "step": 34700
    },
    {
      "epoch": 2.3430767674724384,
      "grad_norm": 12.445003509521484,
      "learning_rate": 4.3834383438343835e-06,
      "loss": 0.4321,
      "step": 34750
    },
    {
      "epoch": 2.346448197970399,
      "grad_norm": 8.19129753112793,
      "learning_rate": 4.360936093609362e-06,
      "loss": 0.4079,
      "step": 34800
    },
    {
      "epoch": 2.3498196284683592,
      "grad_norm": 8.468894004821777,
      "learning_rate": 4.3384338433843385e-06,
      "loss": 0.5024,
      "step": 34850
    },
    {
      "epoch": 2.3531910589663196,
      "grad_norm": 4.456660747528076,
      "learning_rate": 4.315931593159317e-06,
      "loss": 0.464,
      "step": 34900
    },
    {
      "epoch": 2.3565624894642796,
      "grad_norm": 7.021911144256592,
      "learning_rate": 4.293429342934294e-06,
      "loss": 0.3896,
      "step": 34950
    },
    {
      "epoch": 2.35993391996224,
      "grad_norm": 8.070642471313477,
      "learning_rate": 4.270927092709271e-06,
      "loss": 0.4818,
      "step": 35000
    },
    {
      "epoch": 2.3633053504602004,
      "grad_norm": 8.271153450012207,
      "learning_rate": 4.248424842484249e-06,
      "loss": 0.4579,
      "step": 35050
    },
    {
      "epoch": 2.3666767809581604,
      "grad_norm": 7.262307167053223,
      "learning_rate": 4.225922592259226e-06,
      "loss": 0.4245,
      "step": 35100
    },
    {
      "epoch": 2.3700482114561208,
      "grad_norm": 9.522017478942871,
      "learning_rate": 4.203420342034204e-06,
      "loss": 0.4449,
      "step": 35150
    },
    {
      "epoch": 2.373419641954081,
      "grad_norm": 10.963037490844727,
      "learning_rate": 4.180918091809181e-06,
      "loss": 0.4637,
      "step": 35200
    },
    {
      "epoch": 2.3767910724520416,
      "grad_norm": 11.837740898132324,
      "learning_rate": 4.158415841584159e-06,
      "loss": 0.4718,
      "step": 35250
    },
    {
      "epoch": 2.3801625029500015,
      "grad_norm": 7.979604244232178,
      "learning_rate": 4.135913591359136e-06,
      "loss": 0.408,
      "step": 35300
    },
    {
      "epoch": 2.383533933447962,
      "grad_norm": 10.787622451782227,
      "learning_rate": 4.113411341134114e-06,
      "loss": 0.5201,
      "step": 35350
    },
    {
      "epoch": 2.3869053639459223,
      "grad_norm": 7.072847366333008,
      "learning_rate": 4.0909090909090915e-06,
      "loss": 0.518,
      "step": 35400
    },
    {
      "epoch": 2.3902767944438823,
      "grad_norm": 5.1647233963012695,
      "learning_rate": 4.068406840684069e-06,
      "loss": 0.4437,
      "step": 35450
    },
    {
      "epoch": 2.3936482249418427,
      "grad_norm": 7.981543064117432,
      "learning_rate": 4.045904590459046e-06,
      "loss": 0.4464,
      "step": 35500
    },
    {
      "epoch": 2.397019655439803,
      "grad_norm": 11.798991203308105,
      "learning_rate": 4.023402340234024e-06,
      "loss": 0.519,
      "step": 35550
    },
    {
      "epoch": 2.4003910859377635,
      "grad_norm": 7.022409439086914,
      "learning_rate": 4.000900090009001e-06,
      "loss": 0.46,
      "step": 35600
    },
    {
      "epoch": 2.403762516435724,
      "grad_norm": 6.09977388381958,
      "learning_rate": 3.978397839783979e-06,
      "loss": 0.4892,
      "step": 35650
    },
    {
      "epoch": 2.407133946933684,
      "grad_norm": 8.438097953796387,
      "learning_rate": 3.955895589558956e-06,
      "loss": 0.4667,
      "step": 35700
    },
    {
      "epoch": 2.4105053774316443,
      "grad_norm": 10.49695873260498,
      "learning_rate": 3.933393339333933e-06,
      "loss": 0.4477,
      "step": 35750
    },
    {
      "epoch": 2.4138768079296047,
      "grad_norm": 8.048542976379395,
      "learning_rate": 3.910891089108911e-06,
      "loss": 0.4941,
      "step": 35800
    },
    {
      "epoch": 2.4172482384275646,
      "grad_norm": 5.259366989135742,
      "learning_rate": 3.8883888388838885e-06,
      "loss": 0.5153,
      "step": 35850
    },
    {
      "epoch": 2.420619668925525,
      "grad_norm": 10.249581336975098,
      "learning_rate": 3.865886588658867e-06,
      "loss": 0.451,
      "step": 35900
    },
    {
      "epoch": 2.4239910994234855,
      "grad_norm": 5.259607315063477,
      "learning_rate": 3.8433843384338436e-06,
      "loss": 0.4421,
      "step": 35950
    },
    {
      "epoch": 2.427362529921446,
      "grad_norm": 7.24703311920166,
      "learning_rate": 3.820882088208821e-06,
      "loss": 0.446,
      "step": 36000
    },
    {
      "epoch": 2.430733960419406,
      "grad_norm": 5.38437557220459,
      "learning_rate": 3.7983798379837986e-06,
      "loss": 0.4441,
      "step": 36050
    },
    {
      "epoch": 2.434105390917366,
      "grad_norm": 7.678488731384277,
      "learning_rate": 3.775877587758776e-06,
      "loss": 0.5142,
      "step": 36100
    },
    {
      "epoch": 2.4374768214153266,
      "grad_norm": 12.043170928955078,
      "learning_rate": 3.753375337533754e-06,
      "loss": 0.5232,
      "step": 36150
    },
    {
      "epoch": 2.4408482519132866,
      "grad_norm": 5.679054260253906,
      "learning_rate": 3.7308730873087313e-06,
      "loss": 0.4624,
      "step": 36200
    },
    {
      "epoch": 2.444219682411247,
      "grad_norm": 8.715473175048828,
      "learning_rate": 3.7083708370837084e-06,
      "loss": 0.4934,
      "step": 36250
    },
    {
      "epoch": 2.4475911129092074,
      "grad_norm": 6.516922950744629,
      "learning_rate": 3.6858685868586863e-06,
      "loss": 0.4897,
      "step": 36300
    },
    {
      "epoch": 2.450962543407168,
      "grad_norm": 7.671785831451416,
      "learning_rate": 3.6633663366336635e-06,
      "loss": 0.5098,
      "step": 36350
    },
    {
      "epoch": 2.4543339739051278,
      "grad_norm": 9.382444381713867,
      "learning_rate": 3.6408640864086414e-06,
      "loss": 0.4952,
      "step": 36400
    },
    {
      "epoch": 2.457705404403088,
      "grad_norm": 13.654659271240234,
      "learning_rate": 3.6183618361836185e-06,
      "loss": 0.4227,
      "step": 36450
    },
    {
      "epoch": 2.4610768349010486,
      "grad_norm": 10.846797943115234,
      "learning_rate": 3.595859585958596e-06,
      "loss": 0.4574,
      "step": 36500
    },
    {
      "epoch": 2.464448265399009,
      "grad_norm": 10.014508247375488,
      "learning_rate": 3.5733573357335736e-06,
      "loss": 0.5274,
      "step": 36550
    },
    {
      "epoch": 2.467819695896969,
      "grad_norm": 7.214441776275635,
      "learning_rate": 3.550855085508551e-06,
      "loss": 0.4772,
      "step": 36600
    },
    {
      "epoch": 2.4711911263949293,
      "grad_norm": 8.566774368286133,
      "learning_rate": 3.5283528352835287e-06,
      "loss": 0.5046,
      "step": 36650
    },
    {
      "epoch": 2.4745625568928897,
      "grad_norm": 7.71923828125,
      "learning_rate": 3.5058505850585062e-06,
      "loss": 0.505,
      "step": 36700
    },
    {
      "epoch": 2.47793398739085,
      "grad_norm": 8.19939136505127,
      "learning_rate": 3.4833483348334833e-06,
      "loss": 0.5163,
      "step": 36750
    },
    {
      "epoch": 2.48130541788881,
      "grad_norm": 9.257288932800293,
      "learning_rate": 3.4608460846084613e-06,
      "loss": 0.4719,
      "step": 36800
    },
    {
      "epoch": 2.4846768483867705,
      "grad_norm": 7.690037727355957,
      "learning_rate": 3.4383438343834384e-06,
      "loss": 0.4752,
      "step": 36850
    },
    {
      "epoch": 2.488048278884731,
      "grad_norm": 6.122168064117432,
      "learning_rate": 3.4158415841584164e-06,
      "loss": 0.4643,
      "step": 36900
    },
    {
      "epoch": 2.491419709382691,
      "grad_norm": 5.079925537109375,
      "learning_rate": 3.3933393339333935e-06,
      "loss": 0.477,
      "step": 36950
    },
    {
      "epoch": 2.4947911398806513,
      "grad_norm": 5.526894569396973,
      "learning_rate": 3.370837083708371e-06,
      "loss": 0.4408,
      "step": 37000
    },
    {
      "epoch": 2.4981625703786117,
      "grad_norm": 6.1392107009887695,
      "learning_rate": 3.3483348334833486e-06,
      "loss": 0.4426,
      "step": 37050
    },
    {
      "epoch": 2.501534000876572,
      "grad_norm": 16.313610076904297,
      "learning_rate": 3.325832583258326e-06,
      "loss": 0.4605,
      "step": 37100
    },
    {
      "epoch": 2.5049054313745325,
      "grad_norm": 7.463664531707764,
      "learning_rate": 3.3033303330333037e-06,
      "loss": 0.4599,
      "step": 37150
    },
    {
      "epoch": 2.5082768618724924,
      "grad_norm": 8.743163108825684,
      "learning_rate": 3.280828082808281e-06,
      "loss": 0.4615,
      "step": 37200
    },
    {
      "epoch": 2.511648292370453,
      "grad_norm": 10.595380783081055,
      "learning_rate": 3.2583258325832583e-06,
      "loss": 0.4706,
      "step": 37250
    },
    {
      "epoch": 2.515019722868413,
      "grad_norm": 4.233058452606201,
      "learning_rate": 3.2358235823582363e-06,
      "loss": 0.494,
      "step": 37300
    },
    {
      "epoch": 2.518391153366373,
      "grad_norm": 7.830199241638184,
      "learning_rate": 3.2133213321332134e-06,
      "loss": 0.4387,
      "step": 37350
    },
    {
      "epoch": 2.5217625838643336,
      "grad_norm": 9.21847152709961,
      "learning_rate": 3.1908190819081914e-06,
      "loss": 0.468,
      "step": 37400
    },
    {
      "epoch": 2.525134014362294,
      "grad_norm": 10.16865062713623,
      "learning_rate": 3.1683168316831685e-06,
      "loss": 0.458,
      "step": 37450
    },
    {
      "epoch": 2.5285054448602544,
      "grad_norm": 12.426741600036621,
      "learning_rate": 3.145814581458146e-06,
      "loss": 0.4501,
      "step": 37500
    },
    {
      "epoch": 2.5318768753582144,
      "grad_norm": 8.101338386535645,
      "learning_rate": 3.1233123312331235e-06,
      "loss": 0.4864,
      "step": 37550
    },
    {
      "epoch": 2.5352483058561748,
      "grad_norm": 6.5197367668151855,
      "learning_rate": 3.100810081008101e-06,
      "loss": 0.468,
      "step": 37600
    },
    {
      "epoch": 2.538619736354135,
      "grad_norm": 7.13965368270874,
      "learning_rate": 3.0783078307830786e-06,
      "loss": 0.4846,
      "step": 37650
    },
    {
      "epoch": 2.541991166852095,
      "grad_norm": 6.500498294830322,
      "learning_rate": 3.055805580558056e-06,
      "loss": 0.4957,
      "step": 37700
    },
    {
      "epoch": 2.5453625973500555,
      "grad_norm": 6.009521484375,
      "learning_rate": 3.0333033303330333e-06,
      "loss": 0.4562,
      "step": 37750
    },
    {
      "epoch": 2.548734027848016,
      "grad_norm": 6.507812976837158,
      "learning_rate": 3.0108010801080112e-06,
      "loss": 0.431,
      "step": 37800
    },
    {
      "epoch": 2.5521054583459764,
      "grad_norm": 5.356622219085693,
      "learning_rate": 2.9882988298829884e-06,
      "loss": 0.4374,
      "step": 37850
    },
    {
      "epoch": 2.5554768888439368,
      "grad_norm": 10.341601371765137,
      "learning_rate": 2.9657965796579663e-06,
      "loss": 0.4375,
      "step": 37900
    },
    {
      "epoch": 2.5588483193418967,
      "grad_norm": 5.862038612365723,
      "learning_rate": 2.9432943294329434e-06,
      "loss": 0.4159,
      "step": 37950
    },
    {
      "epoch": 2.562219749839857,
      "grad_norm": 6.515649795532227,
      "learning_rate": 2.920792079207921e-06,
      "loss": 0.4456,
      "step": 38000
    },
    {
      "epoch": 2.565591180337817,
      "grad_norm": 7.096941947937012,
      "learning_rate": 2.8982898289828985e-06,
      "loss": 0.491,
      "step": 38050
    },
    {
      "epoch": 2.5689626108357775,
      "grad_norm": 7.93566370010376,
      "learning_rate": 2.875787578757876e-06,
      "loss": 0.4624,
      "step": 38100
    },
    {
      "epoch": 2.572334041333738,
      "grad_norm": 12.17126750946045,
      "learning_rate": 2.8532853285328536e-06,
      "loss": 0.4686,
      "step": 38150
    },
    {
      "epoch": 2.5757054718316983,
      "grad_norm": 8.442489624023438,
      "learning_rate": 2.830783078307831e-06,
      "loss": 0.4404,
      "step": 38200
    },
    {
      "epoch": 2.5790769023296587,
      "grad_norm": 6.502923011779785,
      "learning_rate": 2.8082808280828083e-06,
      "loss": 0.4481,
      "step": 38250
    },
    {
      "epoch": 2.5824483328276187,
      "grad_norm": 6.845944881439209,
      "learning_rate": 2.7857785778577862e-06,
      "loss": 0.446,
      "step": 38300
    },
    {
      "epoch": 2.585819763325579,
      "grad_norm": 7.806971549987793,
      "learning_rate": 2.7632763276327633e-06,
      "loss": 0.4875,
      "step": 38350
    },
    {
      "epoch": 2.5891911938235395,
      "grad_norm": 7.8184614181518555,
      "learning_rate": 2.7407740774077413e-06,
      "loss": 0.5095,
      "step": 38400
    },
    {
      "epoch": 2.5925626243214994,
      "grad_norm": 7.616759300231934,
      "learning_rate": 2.7182718271827184e-06,
      "loss": 0.4362,
      "step": 38450
    },
    {
      "epoch": 2.59593405481946,
      "grad_norm": 5.427819728851318,
      "learning_rate": 2.695769576957696e-06,
      "loss": 0.4177,
      "step": 38500
    },
    {
      "epoch": 2.5993054853174202,
      "grad_norm": 5.784750938415527,
      "learning_rate": 2.6732673267326735e-06,
      "loss": 0.4789,
      "step": 38550
    },
    {
      "epoch": 2.6026769158153806,
      "grad_norm": 6.21198844909668,
      "learning_rate": 2.650765076507651e-06,
      "loss": 0.4853,
      "step": 38600
    },
    {
      "epoch": 2.6060483463133406,
      "grad_norm": 14.60538387298584,
      "learning_rate": 2.6282628262826286e-06,
      "loss": 0.4874,
      "step": 38650
    },
    {
      "epoch": 2.609419776811301,
      "grad_norm": 10.793039321899414,
      "learning_rate": 2.605760576057606e-06,
      "loss": 0.5191,
      "step": 38700
    },
    {
      "epoch": 2.6127912073092614,
      "grad_norm": 7.030359745025635,
      "learning_rate": 2.5832583258325832e-06,
      "loss": 0.4676,
      "step": 38750
    },
    {
      "epoch": 2.6161626378072214,
      "grad_norm": 5.425146579742432,
      "learning_rate": 2.560756075607561e-06,
      "loss": 0.4074,
      "step": 38800
    },
    {
      "epoch": 2.6195340683051818,
      "grad_norm": 8.568461418151855,
      "learning_rate": 2.5382538253825383e-06,
      "loss": 0.5179,
      "step": 38850
    },
    {
      "epoch": 2.622905498803142,
      "grad_norm": 5.79231071472168,
      "learning_rate": 2.5157515751575163e-06,
      "loss": 0.4653,
      "step": 38900
    },
    {
      "epoch": 2.6262769293011026,
      "grad_norm": 5.963294506072998,
      "learning_rate": 2.4932493249324934e-06,
      "loss": 0.4813,
      "step": 38950
    },
    {
      "epoch": 2.629648359799063,
      "grad_norm": 7.826990127563477,
      "learning_rate": 2.470747074707471e-06,
      "loss": 0.4768,
      "step": 39000
    },
    {
      "epoch": 2.633019790297023,
      "grad_norm": 8.722432136535645,
      "learning_rate": 2.4482448244824485e-06,
      "loss": 0.5187,
      "step": 39050
    },
    {
      "epoch": 2.6363912207949833,
      "grad_norm": 4.984360694885254,
      "learning_rate": 2.425742574257426e-06,
      "loss": 0.4536,
      "step": 39100
    },
    {
      "epoch": 2.6397626512929437,
      "grad_norm": 5.687260627746582,
      "learning_rate": 2.4032403240324035e-06,
      "loss": 0.4893,
      "step": 39150
    },
    {
      "epoch": 2.6431340817909037,
      "grad_norm": 7.379334449768066,
      "learning_rate": 2.380738073807381e-06,
      "loss": 0.4088,
      "step": 39200
    },
    {
      "epoch": 2.646505512288864,
      "grad_norm": 6.930466651916504,
      "learning_rate": 2.3582358235823586e-06,
      "loss": 0.4391,
      "step": 39250
    },
    {
      "epoch": 2.6498769427868245,
      "grad_norm": 7.516986846923828,
      "learning_rate": 2.3357335733573357e-06,
      "loss": 0.4828,
      "step": 39300
    },
    {
      "epoch": 2.653248373284785,
      "grad_norm": 8.894208908081055,
      "learning_rate": 2.3132313231323133e-06,
      "loss": 0.4521,
      "step": 39350
    },
    {
      "epoch": 2.656619803782745,
      "grad_norm": 5.704118251800537,
      "learning_rate": 2.290729072907291e-06,
      "loss": 0.4336,
      "step": 39400
    },
    {
      "epoch": 2.6599912342807053,
      "grad_norm": 9.617449760437012,
      "learning_rate": 2.2682268226822684e-06,
      "loss": 0.5297,
      "step": 39450
    },
    {
      "epoch": 2.6633626647786657,
      "grad_norm": 6.20076847076416,
      "learning_rate": 2.245724572457246e-06,
      "loss": 0.4385,
      "step": 39500
    },
    {
      "epoch": 2.6667340952766256,
      "grad_norm": 6.77067756652832,
      "learning_rate": 2.2232223222322234e-06,
      "loss": 0.4727,
      "step": 39550
    },
    {
      "epoch": 2.670105525774586,
      "grad_norm": 7.117171764373779,
      "learning_rate": 2.200720072007201e-06,
      "loss": 0.4509,
      "step": 39600
    },
    {
      "epoch": 2.6734769562725464,
      "grad_norm": 7.46077299118042,
      "learning_rate": 2.1782178217821785e-06,
      "loss": 0.4218,
      "step": 39650
    },
    {
      "epoch": 2.676848386770507,
      "grad_norm": 10.907143592834473,
      "learning_rate": 2.155715571557156e-06,
      "loss": 0.4659,
      "step": 39700
    },
    {
      "epoch": 2.6802198172684673,
      "grad_norm": 6.2732133865356445,
      "learning_rate": 2.1332133213321336e-06,
      "loss": 0.4478,
      "step": 39750
    },
    {
      "epoch": 2.683591247766427,
      "grad_norm": 5.620428085327148,
      "learning_rate": 2.1107110711071107e-06,
      "loss": 0.4698,
      "step": 39800
    },
    {
      "epoch": 2.6869626782643876,
      "grad_norm": 7.576155185699463,
      "learning_rate": 2.0882088208820882e-06,
      "loss": 0.4683,
      "step": 39850
    },
    {
      "epoch": 2.690334108762348,
      "grad_norm": 6.061690330505371,
      "learning_rate": 2.0657065706570658e-06,
      "loss": 0.4777,
      "step": 39900
    },
    {
      "epoch": 2.693705539260308,
      "grad_norm": 8.806295394897461,
      "learning_rate": 2.0432043204320433e-06,
      "loss": 0.4438,
      "step": 39950
    },
    {
      "epoch": 2.6970769697582684,
      "grad_norm": 6.270040035247803,
      "learning_rate": 2.020702070207021e-06,
      "loss": 0.4437,
      "step": 40000
    },
    {
      "epoch": 2.700448400256229,
      "grad_norm": 6.571212291717529,
      "learning_rate": 1.9981998199819984e-06,
      "loss": 0.4872,
      "step": 40050
    },
    {
      "epoch": 2.703819830754189,
      "grad_norm": 9.409923553466797,
      "learning_rate": 1.975697569756976e-06,
      "loss": 0.4951,
      "step": 40100
    },
    {
      "epoch": 2.707191261252149,
      "grad_norm": 7.0518646240234375,
      "learning_rate": 1.9531953195319535e-06,
      "loss": 0.4803,
      "step": 40150
    },
    {
      "epoch": 2.7105626917501096,
      "grad_norm": 7.417196750640869,
      "learning_rate": 1.930693069306931e-06,
      "loss": 0.4622,
      "step": 40200
    },
    {
      "epoch": 2.71393412224807,
      "grad_norm": 11.50953197479248,
      "learning_rate": 1.9081908190819086e-06,
      "loss": 0.4446,
      "step": 40250
    },
    {
      "epoch": 2.71730555274603,
      "grad_norm": 7.621794700622559,
      "learning_rate": 1.8856885688568857e-06,
      "loss": 0.4669,
      "step": 40300
    },
    {
      "epoch": 2.7206769832439903,
      "grad_norm": 7.624521732330322,
      "learning_rate": 1.8631863186318632e-06,
      "loss": 0.4358,
      "step": 40350
    },
    {
      "epoch": 2.7240484137419507,
      "grad_norm": 6.70578145980835,
      "learning_rate": 1.8406840684068408e-06,
      "loss": 0.438,
      "step": 40400
    },
    {
      "epoch": 2.727419844239911,
      "grad_norm": 4.782005310058594,
      "learning_rate": 1.8181818181818183e-06,
      "loss": 0.4126,
      "step": 40450
    },
    {
      "epoch": 2.7307912747378715,
      "grad_norm": 6.107590675354004,
      "learning_rate": 1.7956795679567958e-06,
      "loss": 0.4522,
      "step": 40500
    },
    {
      "epoch": 2.7341627052358315,
      "grad_norm": 7.387401580810547,
      "learning_rate": 1.7731773177317732e-06,
      "loss": 0.4747,
      "step": 40550
    },
    {
      "epoch": 2.737534135733792,
      "grad_norm": 7.0664825439453125,
      "learning_rate": 1.7506750675067507e-06,
      "loss": 0.4763,
      "step": 40600
    },
    {
      "epoch": 2.740905566231752,
      "grad_norm": 6.038769245147705,
      "learning_rate": 1.7281728172817282e-06,
      "loss": 0.4048,
      "step": 40650
    },
    {
      "epoch": 2.7442769967297123,
      "grad_norm": 7.197112560272217,
      "learning_rate": 1.7056705670567058e-06,
      "loss": 0.477,
      "step": 40700
    },
    {
      "epoch": 2.7476484272276727,
      "grad_norm": 6.023144721984863,
      "learning_rate": 1.6831683168316833e-06,
      "loss": 0.4152,
      "step": 40750
    },
    {
      "epoch": 2.751019857725633,
      "grad_norm": 5.325286865234375,
      "learning_rate": 1.6606660666066606e-06,
      "loss": 0.495,
      "step": 40800
    },
    {
      "epoch": 2.7543912882235935,
      "grad_norm": 8.234106063842773,
      "learning_rate": 1.6381638163816382e-06,
      "loss": 0.376,
      "step": 40850
    },
    {
      "epoch": 2.7577627187215534,
      "grad_norm": 8.222960472106934,
      "learning_rate": 1.6156615661566157e-06,
      "loss": 0.3956,
      "step": 40900
    },
    {
      "epoch": 2.761134149219514,
      "grad_norm": 10.042754173278809,
      "learning_rate": 1.5931593159315933e-06,
      "loss": 0.4491,
      "step": 40950
    },
    {
      "epoch": 2.7645055797174742,
      "grad_norm": 5.319066047668457,
      "learning_rate": 1.5706570657065708e-06,
      "loss": 0.4383,
      "step": 41000
    }
  ],
  "logging_steps": 50,
  "max_steps": 44490,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.148973916191782e+18,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
