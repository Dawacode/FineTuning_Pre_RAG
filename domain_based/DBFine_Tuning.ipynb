{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f84d52-21dd-476e-9034-916a625a0f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install unsloth vllm\n",
    "%pip install bitsandbytes accelerate xformers peft trl triton cut_cross_entropy unsloth_zoo\n",
    "%pip install sentencepiece protobuf datasets huggingface_hub hf_transfer setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a7d25-5ab2-4811-ac38-81d6e124a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52631a4-c275-4330-b2ae-f166062c2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] \n",
    "\n",
    "local_model_path = \"./saved_models/gemma-3-4b-it\"\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 2048, \n",
    "    load_in_4bit = True,  \n",
    "    load_in_8bit = False, \n",
    "    full_finetuning = False,\n",
    ")\n",
    "model.save_pretrained(local_model_path)\n",
    "tokenizer.save_pretrained(local_model_path)\n",
    "print(f\"Model saved to: {local_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc7e30-da94-4b7e-8ce1-48eabe80ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, \n",
    "    finetune_language_layers   = True,  \n",
    "    finetune_attention_modules = True,  \n",
    "    finetune_mlp_modules       = True,  \n",
    "\n",
    "    r = 8,          \n",
    "    lora_alpha = 8,  \n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164bbeb-abf5-48a1-a97e-8dc21a29ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"./Last_clean_swedish_updated2.jsonl\")\n",
    "\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]  \n",
    "val_dataset = dataset[\"test\"]   \n",
    "print(f\"Training set size: {len(train_dataset)} examples\")\n",
    "print(f\"Validation set size: {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80810e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=50,\n",
    "    output_dir=\"outputs_gemma3\",\n",
    "    run_name=\"legal_gemma3_finetuning_h100\",\n",
    "    logging_dir=\"wandb_logs_gemma3\",\n",
    "    report_to=\"wandb\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",    \n",
    "    eval_strategy=\"no\",\n",
    "    eval_steps=None,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=None,  \n",
    "    seed=42,\n",
    "    logging_strategy=\"steps\",  \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=None,\n",
    "    args=sft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction((79.109 - 8) / 79.109, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Unsloth Torch 2.1.2",
   "language": "python",
   "name": "unsloth-torch21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
